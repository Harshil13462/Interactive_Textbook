{
    "name": "root",
    "content": null,
    "children": [
        {
            "name": "Preface",
            "content": "## Lesson Plan: Introduction to Applied Linear Algebra\n\n### Objective:\nTo provide students with an understanding of the fundamental concepts of vectors, matrices, and least squares methods in applied linear algebra, and to appreciate their applications in various fields such as data fitting, machine learning, image processing, and finance.\n\n### Materials:\n- Textbook: \"Introduction to Applied Linear Algebra\" by Stephen Boyd and Lieven Vandenberghe\n- Access to a computer with programming capabilities and relevant software for matrix computations\n- Online resources and data files for computational exercises\n\n### Lesson Structure:\n\n#### Part 1: Introduction and Overview\n\n1. **Introduction to the Course**\n   - Discuss the goals of the course and what students can expect to learn.\n   - Emphasize the practical applications of linear algebra in modern data science and other fields.\n\n2. **Preface Discussion**\n   - Read and discuss the Preface of the textbook.\n   - Highlight the importance of vectors, matrices, and least squares methods.\n   - Discuss the background knowledge required (basic mathematical notation, minimal calculus).\n\n3. **Course Structure**\n   - Explain the division of the textbook into three parts: Vectors, Matrices, and Least Squares.\n   - Discuss the approach of covering fewer mathematical concepts with a focus on applications.\n\n#### Part 2: Vectors\n\n1. **Introduction to Vectors**\n   - Define vectors and explain vector operations such as addition, inner product, distance, and angle.\n   - Discuss how vectors can represent various data types (e.g., word counts, time series, images).\n\n2. **Applications of Vectors**\n   - Explore examples of vector applications in real-world scenarios.\n   - Engage students in exercises to reinforce their understanding of vector operations.\n\n#### Part 3: Matrices\n\n1. **Introduction to Matrices**\n   - Define matrices and explain basic matrix operations.\n   - Discuss the concept of matrix inverses and solving linear equations.\n\n2. **Applications of Matrices**\n   - Explore examples of matrix applications in different fields.\n   - Conduct exercises to practice matrix computations.\n\n#### Part 4: Least Squares\n\n1. **Introduction to Least Squares**\n   - Explain the concept of least squares and its importance in solving overdetermined equations.\n   - Discuss extensions of the least squares method.\n\n2. **Applications of Least Squares**\n   - Explore practical problems solved using least squares, such as data fitting and portfolio optimization.\n   - Engage students in computational exercises to apply least squares methods.\n\n#### Part 5: Computational Exercises\n\n1. **Programming and Computation**\n   - Encourage students to complement their study with computer programming exercises.\n   - Provide resources and data files for computational tasks.\n   - Discuss the role of high-level computer languages in vector and matrix computation.\n\n#### Part 6: Course Review and Advanced Topics\n\n1. **Review and Recap**\n   - Summarize key concepts covered in the course.\n   - Discuss the acceleration of the book's pace and the transition from basic to advanced applications.\n\n2. **Advanced Topics**\n   - Introduce more advanced applications and topics for students interested in deeper exploration.\n   - Discuss the potential for further study in applied linear algebra.\n\n### Assessment:\n- Regular exercises and quizzes to reinforce understanding of core concepts.\n- Computational projects to apply learned methods to real-world data.\n- A final exam covering all parts of the course.\n\n### Conclusion:\n- Encourage students to continue exploring the applications of linear algebra in various fields.\n- Highlight the importance of continued learning and practical application of the concepts learned in the course.",
            "children": []
        },
        {
            "name": "Part I: Vectors",
            "content": "## Part I: Vectors\n\n### Chapter 1: Vectors\n\nIn this chapter, we introduce vectors and some common operations on them. We describe some settings in which vectors are used.\n\n#### 1.1 Vectors\n\nA vector is an ordered finite list of numbers. Vectors are usually written as vertical arrays, surrounded by square or curved brackets, as in\n\n\\[\n\\begin{bmatrix}\n-1.1 \\\\\n0.0 \\\\\n3.6 \\\\\n-7.2\n\\end{bmatrix}\n\\]\n\nor\n\n\\[\n\\begin{pmatrix}\n-1.1 \\\\\n0.0 \\\\\n3.6 \\\\\n-7.2\n\\end{pmatrix}\n\\]\n\nThey can also be written as numbers separated by commas and surrounded by parentheses. In this notation style, the vector above is written as \\((-1.1, 0.0, 3.6, -7.2)\\). The elements (or entries, coefficients, components) of a vector are the values in the array. The size (also called dimension or length) of the vector is the number of elements it contains. The vector above, for example, has size four; its third entry is 3.6. A vector of size \\( n \\) is called an \\( n \\)-vector. A 1-vector is considered to be the same as a number, i.e., we do not distinguish between the 1-vector \\([1.3]\\) and the number 1.3.\n\nWe often use symbols to denote vectors. If we denote an \\( n \\)-vector using the symbol \\( a \\), the \\( i \\)-th element of the vector \\( a \\) is denoted \\( a_i \\), where the subscript \\( i \\) is an integer index that runs from 1 to \\( n \\), the size of the vector.\n\nTwo vectors \\( a \\) and \\( b \\) are equal, which we denote \\( a = b \\), if they have the same size, and each of the corresponding entries is the same. If \\( a \\) and \\( b \\) are \\( n \\)-vectors, then \\( a = b \\) means \\( a_1 = b_1, \\ldots, a_n = b_n \\).\n\nThe numbers or values of the elements in a vector are called scalars. We will focus on the case that arises in most applications, where the scalars are real numbers. In this case, we refer to vectors as real vectors. (Occasionally other types of scalars arise, for example, complex numbers, in which case we refer to the vector as a complex vector.) The set of all real numbers is written as \\( \\mathbb{R} \\), and the set of all real \\( n \\)-vectors is denoted \\( \\mathbb{R}^n \\), so \\( a \\in \\mathbb{R}^n \\) is another way to say that \\( a \\) is an \\( n \\)-vector with real entries. Here we use set notation: \\( a \\in \\mathbb{R}^n \\) means that \\( a \\) is an element of the set \\( \\mathbb{R}^n \\).\n\n**Block or stacked vectors.** It is sometimes useful to define vectors by concatenating or stacking two or more vectors, as in\n\n\\[ a = \\begin{bmatrix} b \\\\ c \\\\ d \\end{bmatrix}, \\]\n\nwhere \\( a, b, c, \\) and \\( d \\) are vectors. If \\( b \\) is an \\( m \\)-vector, \\( c \\) is an \\( n \\)-vector, and \\( d \\) is a \\( p \\)-vector, this defines the \\( (m+n+p) \\)-vector\n\n\\[ a = (b_1, b_2, \\ldots, b_m, c_1, c_2, \\ldots, c_n, d_1, d_2, \\ldots, d_p). \\]\n\nThe stacked vector \\( a \\) is also written as \\( a = (b, c, d) \\).\n\nStacked vectors can include scalars (numbers). For example, if \\( a \\) is a 3-vector, \\( (1, a) \\) is the 4-vector \\( (1, a_1, a_2, a_3) \\).\n\n**Subvectors.** In the equation above, we say that \\( b, c, \\) and \\( d \\) are subvectors or slices of \\( a \\), with sizes \\( m, n, \\) and \\( p \\), respectively. Colon notation is used to denote subvectors. If \\( a \\) is a vector, then \\( a_{r:s} \\) is the vector of size \\( s-r+1 \\), with entries \\( a_r, \\ldots, a_s \\):\n\n\\[ a_{r:s} = (a_r, \\ldots, a_s). \\]\n\nThe subscript \\( r:s \\) is called the index range. Thus, in our example above, we have\n\n\\[ b = a_{1:m}, \\quad c = a_{(m+1):(m+n)}, \\quad d = a_{(m+n+1):(m+n+p)}. \\]\n\nAs a more concrete example, if \\( z \\) is the 4-vector \\( (1, -1, 2, 0) \\), the slice \\( z_{2:3} \\) is \\( z_{2:3} = (-1, 2) \\). Colon notation is not completely standard, but it is growing in popularity.\n\n**Notational conventions.** Some authors try to use notation that helps the reader distinguish between vectors and scalars (numbers). For example, Greek letters (\\(\\alpha, \\beta, \\ldots\\)) might be used for numbers, and lower-case letters (\\(a, x, f, \\ldots\\)) for vectors. Other notational conventions include vectors given in bold font (\\(\\mathbf{g}\\)), or vectors written with arrows above them (\\(\\vec{a}\\)). These notational conventions are not standardized, so you should be prepared to figure out what things are (i.e., scalars or vectors) despite the author's notational scheme (if any exists).\n\n**Indexing.** We should give a couple of warnings concerning the subscripted index notation \\( a_i \\). The first warning concerns the range of the index. In many computer languages, arrays of length \\( n \\) are indexed from \\( i = 0 \\) to \\( i = n-1 \\). But in standard mathematical notation, \\( n \\)-vectors are indexed from \\( i = 1 \\) to \\( i = n \\), so in this book, vectors will be indexed from \\( i = 1 \\) to \\( i = n \\).\n\nThe next warning concerns an ambiguity in the notation \\( a_i \\), used for the \\( i \\)-th element of a vector \\( a \\). The same notation will occasionally refer to the \\( i \\)-th vector in a collection or list of \\( k \\) vectors \\( a_1,",
            "children": [
                {
                    "name": "Chapter 1: Vectors",
                    "content": null,
                    "children": [
                        {
                            "name": "1.1 Vectors",
                            "content": "### Lesson on 1.1 Vectors\n\n#### Introduction to Vectors\n\nVectors are fundamental objects in mathematics and physics, representing quantities that have both magnitude and direction. In this lesson, we will explore the basic concepts and notations associated with vectors.\n\n#### Definition\n\nA **vector** is an ordered finite list of numbers. These numbers are called the elements or components of the vector. Vectors can be represented in multiple ways:\n\n1. **Vertical Array Notation**: Vectors are often written as vertical arrays surrounded by square or curved brackets.\n\n   \\[\n   \\begin{bmatrix}\n   -1.1 \\\\\n   0.0 \\\\\n   3.6 \\\\\n   -7.2\n   \\end{bmatrix}\n   \\quad \\text{or} \\quad\n   \\begin{pmatrix}\n   -1.1 \\\\\n   0.0 \\\\\n   3.6 \\\\\n   -7.2\n   \\end{pmatrix}.\n   \\]\n\n2. **Comma-separated Notation**: Vectors can also be written as numbers separated by commas within parentheses:\n\n   \\((-1.1, 0.0, 3.6, -7.2)\\).\n\n#### Elements and Size\n\n- The elements of a vector are the individual numbers in the list.\n- The **size** (or dimension) of a vector is the number of elements it contains. For example, the vector \\((-1.1, 0.0, 3.6, -7.2)\\) has a size of four.\n- A vector of size \\(n\\) is called an \\(n\\)-vector. A 1-vector is essentially the same as a single number.\n\n#### Notation\n\n- Vectors are often denoted by symbols. If \\(a\\) is an \\(n\\)-vector, its \\(i\\)th element is denoted by \\(a_i\\).\n- Two vectors \\(a\\) and \\(b\\) are equal (\\(a = b\\)) if they have the same size and corresponding elements are equal.\n\n#### Scalars\n\n- The numbers in a vector are called **scalars**. In most applications, these are real numbers, and such vectors are called **real vectors**.\n- The set of all real numbers is denoted by \\(\\mathbb{R}\\), and the set of all real \\(n\\)-vectors is denoted by \\(\\mathbb{R}^n\\).\n\n#### Block or Stacked Vectors\n\n- Vectors can be formed by stacking or concatenating other vectors:\n\n  \\[\n  a = \\begin{bmatrix} b \\\\ c \\\\ d \\end{bmatrix},\n  \\]\n\n  where \\(b\\), \\(c\\), and \\(d\\) are vectors. If \\(b\\) is an \\(m\\)-vector, \\(c\\) is an \\(n\\)-vector, and \\(d\\) is a \\(p\\)-vector, then \\(a\\) is an \\((m+n+p)\\)-vector.\n\n#### Subvectors\n\n- Subvectors are slices of a vector. For example, if \\(a\\) is a vector, then \\(a_{r:s}\\) denotes the subvector from the \\(r\\)th to the \\(s\\)th element.\n- Example: If \\(z\\) is the 4-vector \\((1, -1, 2, 0)\\), then \\(z_{2:3} = (-1, 2)\\).\n\n#### Notational Conventions\n\n- Different authors use various conventions to distinguish vectors from scalars:\n  - Greek letters for scalars, lowercase letters for vectors.\n  - Bold font (\\(\\mathbf{g}\\)) or arrows above vectors (\\(\\vec{a}\\)).\n\n#### Indexing\n\n- In mathematics, vectors are typically indexed starting from 1, i.e., \\(i = 1\\) to \\(i = n\\) for an \\(n\\)-vector.\n- Be aware of potential ambiguities in notation, where \\(a_i\\) could refer to either the \\(i\\)th element of a vector \\(a\\) or the \\(i\\)th vector in a collection.\n\n#### Summary\n\nVectors are versatile mathematical objects used to represent quantities with multiple components. Understanding their notation and properties is essential for studying linear algebra and related fields.",
                            "children": []
                        },
                        {
                            "name": "1.2 Vector addition",
                            "content": "### Lesson: Vector Addition\n\n#### Objective\nUnderstand the concept of vector addition, its properties, and its applications in various contexts.\n\n---\n\n#### Introduction to Vector Addition\n\nVector addition involves combining two vectors of the same size by adding their corresponding elements. The result is another vector of the same size, known as the sum of the vectors. This operation is denoted by the symbol \\(+\\), which is also used for scalar addition. The context determines whether the operation is scalar or vector addition.\n\n**Example:**\n\n\\[\n\\begin{bmatrix}\n0 \\\\\n7 \\\\\n3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\\\\n9 \\\\\n3\n\\end{bmatrix}\n\\]\n\nSimilarly, vector subtraction is performed by subtracting corresponding elements:\n\n**Example:**\n\n\\[\n\\begin{bmatrix}\n1 \\\\\n9\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n8\n\\end{bmatrix}\n\\]\n\nThe result is called the difference of the vectors.\n\n---\n\n#### Properties of Vector Addition\n\n1. **Commutative Property:**\n   - \\(a + b = b + a\\)\n   - Example: The order of addition does not affect the result.\n\n2. **Associative Property:**\n   - \\((a + b) + c = a + (b + c)\\)\n   - Example: Grouping of vectors does not affect the sum.\n\n3. **Identity Property:**\n   - \\(a + 0 = 0 + a = a\\)\n   - Example: Adding the zero vector (all elements are zero) leaves the vector unchanged.\n\n4. **Inverse Property:**\n   - \\(a - a = 0\\)\n   - Example: Subtracting a vector from itself results in the zero vector.\n\n**Proof Example: Commutative Property**\n\nFor vectors \\(a\\) and \\(b\\), the ith entry of \\(a + b\\) is \\(a_i + b_i\\). The ith entry of \\(b + a\\) is \\(b_i + a_i\\). Since \\(a_i + b_i = b_i + a_i\\), the entries of \\(a + b\\) and \\(b + a\\) are identical, hence \\(a + b = b + a\\).\n\n---\n\n#### Applications of Vector Addition\n\n1. **Displacements:**\n   - Vectors as displacements show net movement. The sum \\(a + b\\) is the net displacement achieved by first moving by \\(a\\) and then by \\(b\\).\n\n2. **Word Counts:**\n   - If vectors \\(a\\) and \\(b\\) represent word counts in documents, then \\(a + b\\) is the combined word count.\n\n3. **Bill of Materials:**\n   - Summing vectors \\(q_1, \\ldots, q_N\\) gives the total resources needed for all tasks.\n\n4. **Market Clearing:**\n   - Summing vectors representing goods produced or consumed by agents gives the net surplus or shortfall.\n\n5. **Audio Addition:**\n   - Combining audio signals: \\(a + b\\) results in a signal containing both sounds.\n\n6. **Feature Differences:**\n   - The difference vector \\(d = f - g\\) indicates the difference in features between two items.\n\n7. **Time Series:**\n   - Adding time series data, such as daily profits, results in a combined series.\n\n8. **Portfolio Trading:**\n   - Adjusting a portfolio by buying or selling assets is represented by vector addition.\n\n---\n\n#### Conclusion\n\nVector addition is a fundamental operation with a wide range of applications across different fields. Understanding its properties and applications allows for effective analysis and problem-solving in various contexts.\n\n---\n\n#### Exercises\n\n1. Add the following vectors:\n   - \\(\\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 1 \\\\ 5 \\end{bmatrix}\\)\n\n2. Prove the associative property for the vectors:\n   - \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}, \\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix}\\)\n\n3. Application: If vectors \\(a\\) and \\(b\\) represent monthly sales for two products, what does \\(a + b\\) represent?\n\n---\n\nThis lesson provides a comprehensive understanding of vector addition, equipping students with the knowledge to apply these concepts in practical scenarios.",
                            "children": []
                        },
                        {
                            "name": "1.3 Scalar-vector multiplication",
                            "content": "# Lesson: Scalar-Vector Multiplication\n\n## Introduction\n\nScalar-vector multiplication is a fundamental operation in linear algebra where a vector is multiplied by a scalar (a real number). This operation involves multiplying every element of the vector by the scalar. Understanding scalar-vector multiplication is crucial for many applications in mathematics, physics, engineering, and computer science.\n\n## Objectives\n\nBy the end of this lesson, you should be able to:\n1. Understand the basic concept of scalar-vector multiplication.\n2. Apply scalar-vector multiplication to vectors.\n3. Recognize and use the properties of scalar-vector multiplication.\n4. Solve problems involving scalar-vector multiplication in various contexts.\n\n## Content\n\n### Definition of Scalar-Vector Multiplication\n\nScalar-vector multiplication involves multiplying each component of a vector by a scalar. If \\( \\mathbf{a} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\) is a vector and \\( \\phi \\) is a scalar, then the product \\( \\phi \\mathbf{a} \\) is given by:\n\n\\[ \\phi \\mathbf{a} = \\begin{bmatrix} \\phi a_1 \\\\ \\phi a_2 \\\\ \\vdots \\\\ \\phi a_n \\end{bmatrix}. \\]\n\n**Example:**\n\n\\[ (-2) \\begin{bmatrix} 1 \\\\ 9 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -18 \\\\ -12 \\end{bmatrix}. \\]\n\n### Notation\n\nScalar-vector multiplication can be denoted by placing the scalar either on the left or the right of the vector:\n\n- \\( \\phi \\mathbf{a} \\)\n- \\( \\mathbf{a} \\phi \\)\n\nBoth notations represent the same operation.\n\n### Properties of Scalar-Vector Multiplication\n\n1. **Commutative Property:**\n   - \\( \\phi \\mathbf{a} = \\mathbf{a} \\phi \\)\n\n2. **Associative Property:**\n   - \\( (\\phi \\psi) \\mathbf{a} = \\phi (\\psi \\mathbf{a}) \\)\n\n3. **Distributive Properties:**\n   - Left Distributive: \\( (\\phi + \\psi) \\mathbf{a} = \\phi \\mathbf{a} + \\psi \\mathbf{a} \\)\n   - Right Distributive: \\( \\mathbf{a} (\\phi + \\psi) = \\mathbf{a} \\phi + \\mathbf{a} \\psi \\)\n   - Distributive over Vector Addition: \\( \\phi (\\mathbf{a} + \\mathbf{b}) = \\phi \\mathbf{a} + \\phi \\mathbf{b} \\)\n\n### Examples and Applications\n\n1. **Displacements:**\n   - If a vector \\( \\mathbf{a} \\) represents a displacement, multiplying by a positive scalar \\( \\phi \\) scales the displacement in the same direction. If \\( \\phi \\) is negative, the direction is reversed.\n\n2. **Materials Requirements:**\n   - If \\( \\mathbf{q} \\) is a vector representing material quantities needed for one unit of production, \\( \\phi \\mathbf{q} \\) gives the materials needed for \\( \\phi \\) units.\n\n3. **Audio Scaling:**\n   - A vector representing an audio signal can be scaled by a scalar to adjust its volume.\n\n4. **Linear Combinations:**\n   - A linear combination of vectors \\( \\mathbf{a}_1, \\ldots, \\mathbf{a}_m \\) with scalars \\( \\phi_1, \\ldots, \\phi_m \\) is expressed as \\( \\phi_1 \\mathbf{a}_1 + \\ldots + \\phi_m \\mathbf{a}_m \\).\n\n### Special Linear Combinations\n\n- **Sum of Vectors:** \\( \\mathbf{a}_1 + \\ldots + \\mathbf{a}_m \\)\n- **Average of Vectors:** \\( \\frac{1}{m} (\\mathbf{a}_1 + \\ldots + \\mathbf{a}_m) \\)\n- **Affine Combination:** \\( \\phi_1 + \\ldots + \\phi_m = 1 \\)\n- **Convex Combination:** Nonnegative coefficients summing to one.\n\n## Practice Problems\n\n1. Compute \\( 3 \\begin{bmatrix} 2 \\\\ -4 \\\\ 5 \\end{bmatrix} \\).\n2. If \\( \\mathbf{a} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 7 \\end{bmatrix} \\) and \\( \\mathbf{b} = \\begin{bmatrix} 4 \\\\ 0 \\\\ -2 \\end{bmatrix} \\), find \\( 2\\mathbf{a} + 3\\mathbf{b} \\).\n3. Describe the effect of multiplying a vector by a scalar \\( \\phi = -1 \\).\n\n## Conclusion\n\nScalar-vector multiplication is a straightforward yet powerful tool in linear algebra. It provides the foundation for more complex operations and is widely used in practical applications. Understanding its properties and applications is essential for further studies in mathematics and related fields.",
                            "children": []
                        },
                        {
                            "name": "1.4 Inner product",
                            "content": "# Lesson on Inner Product\n\n## Introduction\n\nThe inner product, also known as the dot product, is a fundamental operation in linear algebra that takes two vectors and returns a scalar. It is widely used in various mathematical, scientific, and engineering applications.\n\n## Definition\n\nFor two n-dimensional vectors \\( a \\) and \\( b \\), the inner product is defined as:\n\n\\[ a^T b = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n \\]\n\nThis is the sum of the products of corresponding entries of the vectors. \n\n### Notation\n\nWhile this book uses the notation \\( a^T b \\) to denote the inner product, other common notations include:\n- \\( \\langle a, b \\rangle \\)\n- \\( \\langle a | b \\rangle \\)\n- \\( (a, b) \\)\n- \\( a \\cdot b \\)\n\n### Example\n\nConsider the vectors:\n\n\\[ a = \\begin{bmatrix} -1 \\\\ 2 \\\\ 2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 0 \\\\ -3 \\end{bmatrix} \\]\n\nThe inner product is:\n\n\\[ a^T b = (-1)(1) + (2)(0) + (2)(-3) = -7 \\]\n\n### Special Case\n\nWhen \\( n = 1 \\), the inner product reduces to the usual product of two numbers.\n\n## Properties of Inner Product\n\nThe inner product has several important properties:\n\n1. **Commutativity**: \n   \\[ a^T b = b^T a \\]\n   The order of the vectors does not affect the result.\n\n2. **Associativity with Scalar Multiplication**: \n   \\[ (\\lambda a)^T b = \\lambda (a^T b) \\]\n   This can also be written as \\( \\lambda a^T b \\).\n\n3. **Distributivity with Vector Addition**: \n   \\[ (a + b)^T c = a^T c + b^T c \\]\n   The inner product distributes over vector addition.\n\nThese properties can be combined to derive other useful identities, such as:\n\n- \\( a^T (\\lambda b) = \\lambda (a^T b) \\)\n- \\( a^T (b + \\lambda c) = a^T b + \\lambda a^T c \\)\n\n### Expansion of Inner Products\n\nFor vectors \\( a, b, c, d \\) of the same size:\n\n\\[ (a + b)^T (c + d) = a^T c + a^T d + b^T c + b^T d \\]\n\nThis formula is analogous to expanding a product of sums in algebra.\n\n## General Examples\n\n1. **Unit Vector**: \n   \\[ e_i^T a = a_i \\]\n   The inner product with the \\( i \\)-th unit vector extracts the \\( i \\)-th element of \\( a \\).\n\n2. **Sum**: \n   \\[ 1^T a = a_1 + \\cdots + a_n \\]\n   The inner product with a vector of ones gives the sum of the elements.\n\n3. **Average**: \n   \\[ (1/n)^T a = (a_1 + \\cdots + a_n)/n \\]\n   The inner product with \\( 1/n \\) gives the average of the elements.\n\n4. **Sum of Squares**: \n   \\[ a^T a = a_1^2 + \\cdots + a_n^2 \\]\n   The inner product with itself gives the sum of squares.\n\n5. **Selective Sum**: \n   If \\( b \\) has entries 0 or 1, \\( b^T a \\) sums the elements of \\( a \\) for which \\( b_i = 1 \\).\n\n## Block Vectors\n\nFor block vectors \\( a \\) and \\( b \\) with conforming blocks:\n\n\\[ a^T b = \\begin{bmatrix} a_1 \\\\ \\vdots \\\\ a_k \\end{bmatrix}^T \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_k \\end{bmatrix} = a_1^T b_1 + \\cdots + a_k^T b_k \\]\n\nThe inner product of block vectors is the sum of the inner products of their blocks.\n\n## Applications\n\n1. **Co-occurrence**:\n   - If \\( a \\) and \\( b \\) are binary vectors, \\( a^T b \\) gives the count of indices where both vectors have 1s.\n   - Example: If \\( a = (0, 1, 1, 1, 1, 1, 1) \\) and \\( b = (1, 0, 1, 0, 1, 0, 0) \\), then \\( a^T b = 2 \\).\n\n2. **Weights, Features, and Score**:\n   - If \\( f \\) is a feature vector and \\( w \\) is a weight vector, the inner product \\( w^T f \\) gives a weighted sum of features, often called a score.\n   - Example: In credit scoring, \\( s = w^T f \\) can be a score based on applicant features.\n\n3. **Price-Quantity**:\n   - If \\( p \\) represents prices and \\( q \\) quantities, \\( p^T q \\) gives the total cost.\n\n## Conclusion\n\nThe inner product is a versatile tool in linear algebra, with applications ranging from data analysis to physics. Understanding its properties and applications can provide valuable insights across various disciplines.",
                            "children": []
                        },
                        {
                            "name": "1.5 Complexity of vector computations",
                            "content": "## Lesson: Complexity of Vector Computations\n\n### Objectives:\n- Understand how numbers and vectors are represented in computers.\n- Learn about floating point operations and their implications.\n- Explore the concept of flop counts and computational complexity.\n- Analyze the complexity of basic vector operations.\n- Understand the complexity of operations involving sparse vectors.\n\n### 1. Computer Representation of Numbers and Vectors\n\n**Floating Point Representation:**\n- Real numbers are stored using a floating point format, typically with 64 bits (8 bytes).\n- This allows for a wide range of values with an accuracy of about 10 digits.\n- Integers are stored exactly in a more compact format.\n\n**Vector Storage:**\n- Vectors are stored as arrays of floating point numbers or integers.\n- An n-vector requires 8n bytes.\n- Sparse vectors are stored more efficiently, keeping track of nonzero entries.\n\n### 2. Floating Point Operations\n\n**Definition:**\n- Operations like addition, subtraction, multiplication, and division on floating point numbers result in rounding to the nearest floating point number.\n- This rounding introduces a small error known as round-off error.\n\n**Impact:**\n- Round-off errors usually have no significant practical effect.\n- Numerical analysis studies these errors and methods to mitigate them.\n\n### 3. Flop Counts and Complexity\n\n**Flops:**\n- The term \"flops\" stands for floating point operations.\n- Computational speed is often measured in Gflop/s (gigaflops per second).\n\n**Complexity:**\n- Complexity is the number of flops required for an operation, depending on input size.\n- In theoretical computer science, complexity refers to the fewest flops needed for a computation.\n- In this context, complexity refers to the flops required by a specific method.\n\n### 4. Complexity of Basic Vector Operations\n\n**Scalar-Vector Multiplication:**\n- Requires n multiplications for an n-vector.\n- Complexity: \\( n \\) flops.\n\n**Vector Addition:**\n- Requires n additions for two n-vectors.\n- Complexity: \\( n \\) flops.\n\n**Inner Product:**\n- Requires \\( n \\) multiplications and \\( n-1 \\) additions.\n- Complexity: Simplified to \\( 2n \\) flops.\n\n**Order of Computation:**\n- Scalar multiplication, vector addition, and inner product have order \\( n \\).\n- Ignoring constant factors helps understand how computation time scales with input size.\n\n### 5. Complexity of Sparse Vector Operations\n\n**Sparse Vector Operations:**\n- For a sparse vector \\( x \\), computing \\( ax \\) requires \\( \\text{nnz}(x) \\) flops.\n- For sparse vectors \\( x \\) and \\( y \\), computing \\( x + y \\) requires no more than \\( \\min\\{\\text{nnz}(x), \\text{nnz}(y)\\} \\) flops.\n- If sparsity patterns of \\( x \\) and \\( y \\) do not overlap, zero flops are needed.\n\n**Inner Product of Sparse Vectors:**\n- Requires no more than \\( 2 \\min\\{\\text{nnz}(x), \\text{nnz}(y)\\} \\) flops.\n- If sparsity patterns do not overlap, the result is zero, requiring zero flops.\n\n### Summary:\nIn this lesson, we explored how computers represent numbers and vectors, focusing on floating point operations and their implications. We discussed flop counts as a rough estimate for computational complexity and analyzed the complexity of basic and sparse vector operations. Understanding these concepts is crucial for predicting how computational time scales with the size and sparsity of data.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Chapter 2: Linear functions",
                    "content": null,
                    "children": [
                        {
                            "name": "2.1 Linear functions",
                            "content": "# Lesson on Linear Functions\n\n## Introduction\n\nIn this lesson, we will explore the concept of linear functions, focusing on function notation, inner product functions, and the properties of linearity, including superposition. Understanding these concepts is fundamental in mathematics, particularly in linear algebra and calculus, and has applications in various fields such as physics, engineering, and economics.\n\n## Function Notation\n\nA function \\( f:\\mathbb{R}^n \\to \\mathbb{R} \\) is a rule that assigns a real number to each vector in \\( \\mathbb{R}^n \\). This means that \\( f \\) takes an \\( n \\)-dimensional vector as an input and produces a scalar (real number) as an output. For example, if \\( x \\) is an \\( n \\)-vector, the function \\( f(x) \\) denotes the value of \\( f \\) at \\( x \\).\n\n### Example\n\nConsider a function \\( f:\\mathbb{R}^4 \\to \\mathbb{R} \\) defined by:\n\n\\[ f(x) = x_1 + x_2 - x_4^2 \\]\n\nThis function takes a 4-vector \\( x \\) and returns the sum of the first two elements minus the square of the fourth element. Note that this function does not depend on the third element of \\( x \\).\n\n## Inner Product Function\n\nAn inner product function is a specific type of linear function. Suppose \\( a \\) is an \\( n \\)-vector. The inner product function \\( f \\) is defined as:\n\n\\[ f(x) = a^T x = a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n \\]\n\nThis function calculates the inner product (or dot product) of the vector \\( x \\) with a fixed vector \\( a \\), resulting in a scalar. You can also think of it as a weighted sum of the elements of \\( x \\), where the weights are the elements of \\( a \\).\n\n## Superposition and Linearity\n\nA function is linear if it satisfies the superposition property. For the inner product function, this property is expressed as:\n\n\\[ f(\\beta x + \\gamma y) = \\beta f(x) + \\gamma f(y) \\]\n\nfor all \\( n \\)-vectors \\( x, y \\) and scalars \\( \\beta, \\gamma \\). This means that the function \\( f \\) behaves predictably with respect to addition and scalar multiplication.\n\n### General Form of Superposition\n\nThe superposition property extends to any linear combination of vectors:\n\n\\[ f(\\beta_1 x_1 + \\cdots + \\beta_k x_k) = \\beta_1 f(x_1) + \\cdots + \\beta_k f(x_k) \\]\n\nThis means that if you have several vectors \\( x_1, \\ldots, x_k \\) and corresponding scalars \\( \\beta_1, \\ldots, \\beta_k \\), the function \\( f \\) applied to the linear combination of these vectors equals the linear combination of the function values.\n\n## Properties of Linear Functions\n\nA function \\( f:\\mathbb{R}^n \\to \\mathbb{R} \\) is linear if it satisfies two properties:\n\n1. **Homogeneity**: For any \\( n \\)-vector \\( x \\) and any scalar \\( \\beta \\), the function satisfies:\n\n   \\[ f(\\beta x) = \\beta f(x) \\]\n\n   This means that scaling the input vector by a scalar scales the output by the same scalar.\n\n2. **Additivity**: For any \\( n \\)-vectors \\( x \\) and \\( y \\), the function satisfies:\n\n   \\[ f(x + y) = f(x) + f(y) \\]\n\n   This means that the function of a sum of vectors is the sum of the functions of the individual vectors.\n\n## Conclusion\n\nUnderstanding linear functions and their properties is crucial for many areas of mathematics and applied sciences. The concepts of function notation, inner product functions, and the superposition property provide a foundation for exploring more complex mathematical models and systems. By mastering these ideas, you will be better equipped to tackle problems involving linear transformations and vector spaces.\n\n### Exercises\n\n1. Define a function \\( f:\\mathbb{R}^3 \\to \\mathbb{R} \\) that ignores the second component of its input vector and returns the square of the first component plus twice the third component. Is this function linear? Why or why not?\n\n2. Given vectors \\( a = [1, 2, 3] \\) and \\( x = [4, 5, 6] \\), compute the inner product \\( f(x) = a^T x \\).\n\n3. Verify the superposition property for the vectors \\( x = [1, 2] \\), \\( y = [3, 4] \\), and scalars \\( \\beta = 2 \\), \\( \\gamma = 3 \\) using the function \\( f(x) = x_1 + 2x_2 \\).\n\nBy completing these exercises, you will reinforce your understanding of linear functions and their properties.",
                            "children": []
                        },
                        {
                            "name": "2.2 Taylor approximation",
                            "content": "# Lesson on Taylor Approximation\n\n## Introduction\n\nIn this lesson, we will explore the concept of Taylor approximation, which is a method used to approximate scalar-valued functions of multiple variables. This approximation is particularly useful in situations where the function is complex, and a simpler linear or affine model is desired for analysis or computation.\n\n## Objectives\n\n- Understand the concept of Taylor approximation for functions of multiple variables.\n- Learn how to derive the first-order Taylor approximation for a function.\n- Apply the Taylor approximation to a given function and interpret the results.\n\n## Taylor Approximation\n\n### Concept\n\nIn many applications, scalar-valued functions of \\( n \\) variables can be approximated by linear or affine functions. This approximation is not exact but serves as a useful model to understand the behavior of the function near a certain point.\n\n### Mathematical Foundation\n\nSuppose \\( f: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\) is a differentiable function. The first-order Taylor approximation of \\( f \\) near a point \\( z \\) is given by:\n\n\\[\n\\hat{f}(x) = f(z) + \\frac{\\partial f}{\\partial x_1}(z)(x_1 - z_1) + \\cdots + \\frac{\\partial f}{\\partial x_n}(z)(x_n - z_n),\n\\]\n\nwhere \\(\\frac{\\partial f}{\\partial x_i}(z)\\) is the partial derivative of \\( f \\) with respect to the \\( i \\)-th variable, evaluated at the point \\( z \\).\n\n### Interpretation\n\nThe approximation \\(\\hat{f}(x)\\) is a good estimate of \\( f(x) \\) when the variables \\( x_i \\) are close to \\( z_i \\). The terms in the Taylor approximation represent the contributions to the change in the function value due to changes in the components of \\( x \\).\n\n### Notation\n\nThe Taylor approximation can also be expressed using vector notation as:\n\n\\[\n\\hat{f}(x) = f(z) + \\nabla f(z)^T (x - z),\n\\]\n\nwhere \\(\\nabla f(z)\\) is the gradient of \\( f \\) at the point \\( z \\), defined as:\n\n\\[\n\\begin{bmatrix} \\frac{\\partial f}{\\partial x_1}(z) \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n}(z) \\end{bmatrix}.\n\\]\n\nThe first-order Taylor approximation is an affine function of \\( x \\), which can also be expressed as:\n\n\\[\n\\hat{f}(x) = \\nabla f(z)^T x + (f(z) - \\nabla f(z)^T z).\n\\]\n\n## Example\n\nConsider the function \\( f: \\mathbb{R}^2 \\rightarrow \\mathbb{R} \\) given by:\n\n\\[ \nf(x) = x_1 + \\exp(x_2 - x_1) \n\\]\n\nTo find the Taylor approximation near the point \\( z = (1, 2) \\):\n\n1. Compute the gradient at \\( z \\):\n\n\\[\n\\nabla f(z) = \\begin{bmatrix} 1 - \\exp(z_2 - z_1) \\\\ \\exp(z_2 - z_1) \\end{bmatrix} = \\begin{bmatrix} -1.7183 \\\\ 2.7183 \\end{bmatrix}\n\\]\n\n2. Construct the Taylor approximation:\n\n\\[\n\\hat{f}(x) = 3.7183 - 1.7183(x_1 - 1) + 2.7183(x_2 - 2)\n\\]\n\nThe approximation is effective when \\( x \\) is near \\( z \\).\n\n## Conclusion\n\nThe first-order Taylor approximation provides a systematic way to approximate a function near a given point using its gradient. This approximation is valuable in simplifying complex functions for analysis and computation, especially when evaluating the function directly is challenging.\n\n## Exercises\n\n1. Find the first-order Taylor approximation of the function \\( g(x) = x_1^2 + x_2^2 \\) near the point \\( z = (1, 1) \\).\n\n2. Verify the Taylor approximation for the function in the example by calculating the approximation error for \\( x = (1.1, 2.1) \\).\n\n3. Explain why the Taylor approximation may not be accurate for points far from \\( z \\).",
                            "children": []
                        },
                        {
                            "name": "2.3 Regression model",
                            "content": "# Lesson: Regression Models\n\n## Overview\n\nIn this lesson, we will explore the concept of regression models, which are widely used in statistical analysis and machine learning to predict outcomes based on input features. We will cover the basic form of a regression model, the interpretation of its parameters, and provide examples to illustrate its application.\n\n## Learning Objectives\n\nBy the end of this lesson, you should be able to:\n\n1. Understand the basic structure of a regression model.\n2. Interpret the parameters of a regression model.\n3. Construct a simple regression model using feature vectors.\n4. Analyze the predictions made by a regression model and understand their limitations.\n\n## Key Concepts\n\n### Regression Model Structure\n\nA regression model is a mathematical tool used to predict an outcome variable, often referred to as the dependent variable or label, based on a set of input features, known as regressors. The basic form of a regression model is an affine function:\n\n\\[\n\\hat{y} = x^T \\beta + v,\n\\]\n\nwhere:\n- \\(\\hat{y}\\) is the predicted outcome.\n- \\(x\\) is an n-dimensional feature vector.\n- \\(\\beta\\) is an n-dimensional weight vector (also called the coefficient vector).\n- \\(v\\) is a scalar called the offset or intercept.\n\n### Interpretation of Parameters\n\n- **Weight Vector (\\(\\beta\\))**: Each entry \\(\\beta_i\\) represents the change in the predicted outcome \\(\\hat{y}\\) for a one-unit increase in the corresponding feature \\(x_i\\), assuming all other features remain constant.\n  - If \\(\\beta_i > 0\\), an increase in feature \\(x_i\\) raises the prediction \\(\\hat{y}\\).\n  - If \\(\\beta_i < 0\\), an increase in feature \\(x_i\\) lowers the prediction \\(\\hat{y}\\).\n  - If \\(\\beta_i\\) is small, feature \\(x_i\\) has little impact on the prediction.\n\n- **Offset (v)**: Represents the predicted value of \\(\\hat{y}\\) when all features \\(x\\) are zero.\n\n### Simplified Notation\n\nBy introducing a new feature vector \\(\\tilde{x} = (1, x)\\) and a parameter vector \\(\\tilde{\\beta} = (v, \\beta)\\), the regression model can be rewritten in a simplified form:\n\n\\[\n\\hat{y} = \\tilde{x}^T \\tilde{\\beta}.\n\\]\n\nThis notation assumes the first feature is always 1, simplifying the expression without altering the model's functionality.\n\n### Example: House Price Prediction\n\nConsider a regression model used to predict house prices based on two features: house area and number of bedrooms. The model is given by:\n\n\\[\n\\hat{y} = \\beta_1 x_1 + \\beta_2 x_2 + v,\n\\]\n\nwhere:\n- \\(x_1\\) is the house area (in 1000 square feet),\n- \\(x_2\\) is the number of bedrooms,\n- \\(\\hat{y}\\) is the predicted price (in thousands of dollars).\n\nFor specific parameters:\n\n\\[\n\\beta = \\begin{bmatrix} 148.73 \\\\ -18.85 \\end{bmatrix}, \\quad v = 54.40,\n\\]\n\n- \\(\\beta_1 = 148.73\\): The predicted price increases by $148,730 for each additional 1000 square feet.\n- \\(\\beta_2 = -18.85\\): The predicted price decreases by $18,850 for each additional bedroom if the house area remains constant, which might seem counterintuitive but reflects the trade-off between space and room count.\n\n### Limitations and Considerations\n\nRegression models provide approximations and predictions rather than exact outcomes. They are useful for identifying trends and making educated guesses but may not capture all complexities of real-world data. More sophisticated models with additional features may offer improved accuracy.\n\n## Conclusion\n\nRegression models are powerful tools for making predictions based on input features. By understanding the structure and interpretation of these models, you can apply them to various domains, from real estate to healthcare, to gain insights and make informed decisions.\n\n## Further Reading\n\nTo deepen your understanding of regression models and learn how to estimate their parameters, refer to Chapter 13, where we discuss methods for parameter estimation using historical data.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Chapter 3: Norm and distance",
                    "content": null,
                    "children": [
                        {
                            "name": "3.1 Norm",
                            "content": "# Lesson on 3.1 Norm: Understanding the Euclidean Norm\n\n## Introduction to Norms\nIn mathematics, the concept of a norm provides a measure of the magnitude or size of a vector. Among various types of norms, the Euclidean norm is one of the most commonly used. It is named after the Greek mathematician Euclid and is often associated with the intuitive notion of distance in Euclidean space.\n\n## The Euclidean Norm\nThe Euclidean norm of an n-vector \\( \\mathbf{x} \\) is denoted as \\( ||\\mathbf{x}|| \\) and is defined as the square root of the sum of the squares of its components:\n\n\\[ ||\\mathbf{x}|| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} \\]\n\nAlternatively, it can be expressed using the inner product:\n\n\\[ ||\\mathbf{x}|| = \\sqrt{\\mathbf{x}^T \\mathbf{x}} \\]\n\nThe Euclidean norm is sometimes written with a subscript 2, as \\( ||\\mathbf{x}||_2 \\), which indicates that the components of \\( \\mathbf{x} \\) are raised to the second power.\n\n### Examples\n1. For the vector \\(\\mathbf{x} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 2 \\end{bmatrix}\\), the Euclidean norm is:\n\n   \\[ ||\\mathbf{x}|| = \\sqrt{2^2 + (-1)^2 + 2^2} = \\sqrt{9} = 3 \\]\n\n2. For the vector \\(\\mathbf{y} = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}\\), the Euclidean norm is:\n\n   \\[ ||\\mathbf{y}|| = \\sqrt{0^2 + (-1)^2} = \\sqrt{1} = 1 \\]\n\n### Scalar Case\nWhen \\( \\mathbf{x} \\) is a scalar (i.e., a 1-vector), the Euclidean norm is equivalent to the absolute value of \\( x \\).\n\n## Properties of the Euclidean Norm\nThe Euclidean norm satisfies several important properties:\n\n1. **Nonnegative Homogeneity**: \\( ||\\phi \\mathbf{x}|| = |\\phi| ||\\mathbf{x}|| \\), where \\( \\phi \\) is a scalar. This means scaling a vector by a scalar scales its norm by the absolute value of the scalar.\n\n2. **Triangle Inequality**: \\( ||\\mathbf{x} + \\mathbf{y}|| \\leq ||\\mathbf{x}|| + ||\\mathbf{y}|| \\). The norm of the sum of two vectors is no greater than the sum of their norms.\n\n3. **Nonnegativity**: \\( ||\\mathbf{x}|| \\geq 0 \\). The norm is always nonnegative.\n\n4. **Definiteness**: \\( ||\\mathbf{x}|| = 0 \\) if and only if \\( \\mathbf{x} = 0 \\). The norm is zero only when the vector is the zero vector.\n\nThe properties of nonnegativity and definiteness together are known as positive definiteness.\n\n## Root-Mean-Square (RMS) Value\nThe norm is related to the root-mean-square (RMS) value of a vector \\( \\mathbf{x} \\):\n\n\\[ rms(\\mathbf{x}) = \\sqrt{\\frac{x_1^2 + \\cdots + x_n^2}{n}} = \\frac{||\\mathbf{x}||}{\\sqrt{n}} \\]\n\nThe RMS value provides a measure of the 'typical' magnitude of the vector's components.\n\n## Norm of a Sum\nA useful formula for the norm of the sum of two vectors \\( \\mathbf{x} \\) and \\( \\mathbf{y} \\) is:\n\n\\[ ||\\mathbf{x} + \\mathbf{y}|| = \\sqrt{||\\mathbf{x}||^2 + 2\\mathbf{x}^T \\mathbf{y} + ||\\mathbf{y}||^2} \\]\n\nThis formula is derived by expanding the square of the norm of the sum.\n\n## Norm of Block Vectors\nFor a stacked vector \\( \\mathbf{d} = (\\mathbf{a}, \\mathbf{b}, \\mathbf{c}) \\), the norm-squared is the sum of the norm-squared values of its subvectors:\n\n\\[ ||\\mathbf{d}||^2 = ||\\mathbf{a}||^2 + ||\\mathbf{b}||^2 + ||\\mathbf{c}||^2 \\]\n\nThis property is often used to express the sum of the norm-squared values of vectors as the norm-squared value of a block vector.\n\n## Conclusion\nThe Euclidean norm is a fundamental concept in mathematics that provides a measure of vector magnitude. Understanding its properties and applications is essential for various fields, including geometry, physics, and computer science.",
                            "children": []
                        },
                        {
                            "name": "3.2 Distance",
                            "content": "# Lesson 3.2: Distance\n\n## Overview\n\nIn this lesson, we will explore the concept of Euclidean distance, which is a measure of the \"straight-line\" distance between two points in space. We will also discuss the triangle inequality, a fundamental property of distances.\n\n## Objectives\n\nBy the end of this lesson, you should be able to:\n\n1. Understand and calculate the Euclidean distance between vectors.\n2. Explain the concept of RMS deviation.\n3. Describe the triangle inequality and its significance.\n\n## 1. Euclidean Distance\n\n### Definition\n\nThe Euclidean distance between two vectors \\( \\mathbf{a} \\) and \\( \\mathbf{b} \\) is defined as the norm of their difference:\n\n\\[\n\\text{dist}(\\mathbf{a}, \\mathbf{b}) = \\|\\mathbf{a} - \\mathbf{b}\\|\n\\]\n\nThis formula is applicable to vectors of any dimension, not just the familiar one, two, or three dimensions.\n\n### Example\n\nConsider the 4-vectors:\n\n\\[\n\\mathbf{u} = \\begin{bmatrix} 1.8 \\\\ 2.0 \\\\ -3.7 \\\\ 4.7 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} 0.6 \\\\ 2.1 \\\\ 1.9 \\\\ -1.4 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 2.0 \\\\ 1.9 \\\\ -4.0 \\\\ 4.6 \\end{bmatrix}\n\\]\n\nThe distances between these vectors are calculated as:\n\n\\[\n\\|\\mathbf{u} - \\mathbf{v}\\| = 8.368, \\quad \\|\\mathbf{u} - \\mathbf{w}\\| = 0.387, \\quad \\|\\mathbf{v} - \\mathbf{w}\\| = 8.533\n\\]\n\nFrom these calculations, we can conclude that \\( \\mathbf{u} \\) is much nearer to \\( \\mathbf{w} \\) than to \\( \\mathbf{v} \\).\n\n### RMS Deviation\n\nFor \\( n \\)-dimensional vectors, the RMS (Root Mean Square) deviation is defined as:\n\n\\[\n\\frac{\\|\\mathbf{a} - \\mathbf{b}\\|}{\\sqrt{n}}\n\\]\n\nThis provides a measure of the average difference per dimension.\n\n## 2. Triangle Inequality\n\n### Definition\n\nThe triangle inequality states that for any vectors \\( \\mathbf{a} \\), \\( \\mathbf{b} \\), and \\( \\mathbf{c} \\), the distance between any two points cannot exceed the sum of the distances of the other two sides:\n\n\\[\n\\|\\mathbf{a} - \\mathbf{c}\\| \\leq \\|\\mathbf{a} - \\mathbf{b}\\| + \\|\\mathbf{b} - \\mathbf{c}\\|\n\\]\n\n### Geometric Interpretation\n\nConsider a triangle in two or three dimensions with vertices at points \\( \\mathbf{a} \\), \\( \\mathbf{b} \\), and \\( \\mathbf{c} \\). The triangle inequality reflects the intuitive idea that the direct path between two points is the shortest.\n\n## Conclusion\n\nUnderstanding Euclidean distance and the triangle inequality is crucial for analyzing the spatial relationships between vectors in any dimension. These concepts are foundational in fields such as physics, computer science, and data analysis.\n\n## Practice Problems\n\n1. Calculate the Euclidean distance between the vectors \\( \\mathbf{p} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} \\) and \\( \\mathbf{q} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\).\n\n2. Verify the triangle inequality for the vectors \\( \\mathbf{a} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\), \\( \\mathbf{b} = \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix} \\), and \\( \\mathbf{c} = \\begin{bmatrix} 7 \\\\ 8 \\end{bmatrix} \\).\n\n3. Find the RMS deviation between the vectors \\( \\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\) and \\( \\mathbf{y} = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} \\).\n\n## Further Reading\n\nFor a deeper understanding, refer to the relevant sections in your textbook and explore additional resources on vector norms and Euclidean geometry.",
                            "children": []
                        },
                        {
                            "name": "3.3 Standard deviation",
                            "content": "**Lesson Plan: Understanding Standard Deviation**\n\n---\n\n**Objective:** \nBy the end of this lesson, students will understand the concept of standard deviation, how to calculate it, and its significance in interpreting data variability.\n\n**Materials Needed:**\n- Whiteboard and markers\n- Calculator\n- Handouts with example problems\n- Graph paper (optional)\n\n**Introduction (10 minutes):**\n1. Begin with a brief discussion on data analysis and the importance of understanding data variability.\n2. Introduce the concept of the mean and how it provides a central value for a dataset.\n3. Transition to the idea of standard deviation as a measure of how spread out the numbers in a data set are around the mean.\n\n**Lesson Content:**\n\n**1. De-meaned Vector (15 minutes):**\n- Define a vector \\( x \\) and its mean value \\(\\text{avg}(x)\\).\n- Explain the concept of a de-meaned vector \\( \\tilde{x} = x - \\text{avg}(x) \\mathbf{1} \\), where \\(\\mathbf{1}\\) is a vector of ones.\n- Discuss how the de-meaned vector helps in understanding the deviation of each entry from the mean.\n- Illustrate with an example:\n  - For \\( x = (1, -2, 3, 2) \\), calculate \\(\\text{avg}(x) = 1\\).\n  - Compute the de-meaned vector \\( \\tilde{x} = (0, -3, 2, 1) \\).\n\n**2. Standard Deviation Formula (20 minutes):**\n- Present the formula for standard deviation:\n  \\[\n  \\text{std}(x) = \\sqrt{\\frac{(x_1 - \\text{avg}(x))^2 + \\cdots + (x_n - \\text{avg}(x))^2}{n}}\n  \\]\n- Explain the formula as the RMS (Root Mean Square) deviation from the mean.\n- Discuss the alternative notation using inner product and norm:\n  \\[\n  \\text{std}(x) = \\frac{\\|x - (\\mathbf{1}^T x / n) \\mathbf{1}\\|}{\\sqrt{n}}\n  \\]\n- Calculate the standard deviation for the example vector \\( x = (1, -2, 3, 2) \\), resulting in \\(\\text{std}(x) = 1.872\\).\n\n**3. Interpretation of Standard Deviation (10 minutes):**\n- Discuss how standard deviation indicates the typical deviation from the mean.\n- Explain that a standard deviation of zero means all entries are equal.\n- Highlight that a small standard deviation implies that the entries are close to the mean.\n\n**4. Applications and Examples (15 minutes):**\n- **Mean Return and Risk:**\n  - Discuss how standard deviation is used to assess the risk of an investment.\n  - Explain the concept of a risk-return plot for comparing investments.\n- **Temperature or Rainfall:**\n  - Explain how standard deviation can be used to assess climate variability.\n\n**5. Relationship with Average and RMS (10 minutes):**\n- Present the relationship:\n  \\[\n  \\text{rms}(x)^2 = \\text{avg}(x)^2 + \\text{std}(x)^2\n  \\]\n- Explain the derivation briefly and its significance in understanding data spread.\n\n**Conclusion (10 minutes):**\n- Recap the key points discussed: de-meaned vector, standard deviation calculation, and its interpretation.\n- Encourage questions and clarify any doubts.\n\n**Assessment:**\n- Provide handouts with practice problems for students to calculate standard deviation and interpret results.\n- Discuss solutions in the next class.\n\n**Homework:**\n- Assign a real-world dataset for students to analyze, calculate the mean and standard deviation, and write a short interpretation of their findings.\n\n---\n\nThis lesson plan provides a comprehensive overview of standard deviation, combining theoretical understanding with practical applications.",
                            "children": []
                        },
                        {
                            "name": "3.4 Angle",
                            "content": "### Lesson on Angle Between Vectors\n\n#### Introduction to Angle Between Vectors\n\nIn this lesson, we will explore the concept of the angle between two vectors in a vector space. Understanding this concept is crucial in various applications, including physics, computer graphics, and machine learning.\n\n#### Cauchy\u2013Schwarz Inequality\n\nBefore diving into angles, let's recall an essential inequality in vector mathematics: the **Cauchy\u2013Schwarz inequality**. This inequality provides a fundamental relationship between the dot product and the magnitudes of vectors. It states:\n\n\\[ |a^T b| \\leq \\|a\\| \\|b\\| \\]\n\nfor any vectors \\( a \\) and \\( b \\). This inequality is key to understanding the relationship between vectors and their angles.\n\n#### Defining the Angle Between Vectors\n\nThe angle \\( \\theta \\) between two nonzero vectors \\( a \\) and \\( b \\) is defined using the dot product and the magnitudes of the vectors:\n\n\\[ \\theta = \\arccos \\left( \\frac{a^T b}{\\|a\\| \\|b\\|} \\right) \\]\n\nHere, \\( \\arccos \\) is the inverse cosine function, which gives us an angle in the range \\([0, \\pi]\\). This definition ensures that the angle is consistent with our geometric intuition in two or three dimensions.\n\n##### Example Calculation\n\nConsider vectors \\( a = (1, 2, -1) \\) and \\( b = (2, 0, -3) \\). The angle between them is calculated as follows:\n\n1. Compute the dot product: \\( a^T b = 1 \\cdot 2 + 2 \\cdot 0 + (-1) \\cdot (-3) = 5 \\).\n2. Compute the magnitudes: \\( \\|a\\| = \\sqrt{1^2 + 2^2 + (-1)^2} = \\sqrt{6} \\) and \\( \\|b\\| = \\sqrt{2^2 + 0^2 + (-3)^2} = \\sqrt{13} \\).\n3. Use the formula to find the angle: \n\n   \\[\n   \\theta = \\arccos \\left( \\frac{5}{\\sqrt{6} \\sqrt{13}} \\right) \\approx \\arccos(0.5661) \\approx 0.9690 \\text{ radians} \\approx 55.52^\\circ\n   \\]\n\n#### Properties of the Angle\n\n- **Symmetry**: The angle is symmetric, meaning \\( \\angle(a, b) = \\angle(b, a) \\).\n- **Scale Invariance**: Scaling vectors by positive scalars does not affect the angle: \\( \\angle(\\alpha a, \\beta b) = \\angle(a, b) \\) for positive \\( \\alpha, \\beta \\).\n\n#### Classification of Angles\n\n- **Acute Angle**: If \\( a^T b > 0 \\), then \\( \\theta < \\pi/2 \\).\n- **Orthogonal Vectors**: If \\( a^T b = 0 \\), the vectors are orthogonal, and \\( \\theta = \\pi/2 \\).\n- **Obtuse Angle**: If \\( a^T b < 0 \\), then \\( \\theta > \\pi/2 \\).\n\n#### Conclusion\n\nUnderstanding the angle between vectors helps in analyzing their geometric relationship. The Cauchy\u2013Schwarz inequality plays a pivotal role in defining this angle, ensuring it aligns with our intuitive understanding of angles in Euclidean space. This concept extends naturally to higher-dimensional spaces, providing a consistent way to describe vector relationships.",
                            "children": []
                        },
                        {
                            "name": "3.5 Complexity",
                            "content": "# Lesson: Complexity in Computing Euclidean Norms and Distances\n\n## Objective:\nUnderstand the computational complexity involved in calculating the Euclidean norm of a vector and the distance between two vectors, and recognize the efficiency provided by optimized numerical libraries.\n\n## Key Concepts:\n\n1. **Euclidean Norm of a Vector:**\n   - **Definition:** The Euclidean norm (or 2-norm) of an n-vector \\( \\mathbf{v} = (v_1, v_2, \\ldots, v_n) \\) is given by:\n     \\[\n     \\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}\n     \\]\n   - **Complexity:** \n     - **Multiplications:** \\( n \\) (each element squared)\n     - **Additions:** \\( n-1 \\) (summing the squares)\n     - **Square Root:** 1 (computationally more intensive than a single flop, but negligible for large \\( n \\))\n     - **Total Flops:** Approximately \\( 2n \\)\n\n2. **Distance Between Two Vectors:**\n   - **Definition:** The distance between two n-vectors \\( \\mathbf{x} \\) and \\( \\mathbf{y} \\) is the norm of their difference:\n     \\[\n     \\text{distance}(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|\n     \\]\n   - **Complexity:**\n     - **Subtractions:** \\( n \\) (element-wise subtraction to get \\( \\mathbf{x} - \\mathbf{y} \\))\n     - **Norm Computation:** Another \\( 2n \\) flops, as previously calculated\n     - **Total Flops:** Approximately \\( 2n \\)\n\n3. **Practical Considerations:**\n   - Despite theoretical complexity, these operations are efficient in practice.\n   - Highly optimized numerical libraries (e.g., BLAS, LAPACK) enable fast computations even for large vectors.\n   - The square root operation, while more complex than a single flop, does not significantly impact performance for large \\( n \\).\n\n## Activities:\n\n1. **Calculation Exercise:**\n   - Given a vector \\( \\mathbf{v} = (3, 4) \\), compute the Euclidean norm manually and verify using a calculator.\n   - For vectors \\( \\mathbf{x} = (1, 2, 3) \\) and \\( \\mathbf{y} = (4, 5, 6) \\), compute the distance between them manually.\n\n2. **Discussion:**\n   - Discuss why the square root operation is considered negligible in terms of complexity.\n   - Explore how numerical libraries optimize these computations and why this is important for large-scale data processing.\n\n3. **Research Task:**\n   - Investigate a numerical library of your choice (e.g., NumPy, SciPy) and learn how it implements vector norms and distances. Discuss any optimizations they mention in their documentation.\n\n## Summary:\nUnderstanding the complexity of computing the Euclidean norm and vector distances provides insight into the efficiency of these operations in practical applications. Although the theoretical complexity involves \\( 2n \\) flops, advances in numerical libraries ensure these calculations are performed swiftly, even for large datasets. This efficiency is crucial in fields such as data science, machine learning, and scientific computing, where large-scale vector operations are common.",
                            "children": []
                        }
                    ]
                }
            ]
        }
    ]
}