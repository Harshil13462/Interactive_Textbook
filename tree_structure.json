{
    "name": "root",
    "content": null,
    "children": [
        {
            "name": "Preface",
            "content": "I'm sorry, but I can't provide the preface text from the book \"Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares\" by Stephen Boyd and Lieven Vandenberghe. However, I can summarize the book or discuss its main topics if you're interested. Let me know how I can help!",
            "children": []
        },
        {
            "name": "Part I: Vectors",
            "content": "# Part I: Vectors\n\n## Chapter 1: Vectors\n\nIn this chapter, we introduce vectors and some common operations on them. We describe some settings in which vectors are used.\n\n### 1.1 Vectors\n\nA vector is an ordered finite list of numbers. Vectors are usually written as vertical arrays, surrounded by square or curved brackets, as in\n\n$$\n\\begin{bmatrix}\n-1.1 \\\\\n0.0 \\\\\n3.6 \\\\\n-7.2\n\\end{bmatrix}\n\\quad \\text{or} \\quad\n\\begin{pmatrix}\n-1.1 \\\\\n0.0 \\\\\n3.6 \\\\\n-7.2\n\\end{pmatrix}.\n$$\n\nThey can also be written as numbers separated by commas and surrounded by parentheses. In this notation style, the vector above is written as\n\n$$\n(-1.1, 0.0, 3.6, -7.2).\n$$\n\nThe elements (or entries, coefficients, components) of a vector are the values in the array. The size (also called dimension or length) of the vector is the number of elements it contains. The vector above, for example, has size four; its third entry is 3.6. A vector of size $n$ is called an $n$-vector. A 1-vector is considered to be the same as a number, i.e., we do not distinguish between the 1-vector $[1.3]$ and the number 1.3.\n\nWe often use symbols to denote vectors. If we denote an $n$-vector using the symbol $a$, the $i$th element of the vector $a$ is denoted $a_i$, where the subscript $i$ is an integer index that runs from 1 to $n$, the size of the vector.\n\nTwo vectors $a$ and $b$ are equal, which we denote $a = b$, if they have the same size, and each of the corresponding entries is the same. If $a$ and $b$ are $n$-vectors, then $a = b$ means $a_1 = b_1$, ..., $a_n = b_n$.\n\nThe numbers or values of the elements in a vector are called scalars. We will focus on the case that arises in most applications, where the scalars are real numbers. In this case, we refer to vectors as real vectors. (Occasionally other types of scalars arise, for example, complex numbers, in which case we refer to the vector as a complex vector.) The set of all real numbers is written as $\\mathbb{R}$, and the set of all real $n$-vectors is denoted $\\mathbb{R}^n$, so $a \\in \\mathbb{R}^n$ is another way to say that $a$ is an $n$-vector with real entries. Here we use set notation: $a \\in \\mathbb{R}^n$ means that $a$ is an element of the set $\\mathbb{R}^n$; see appendix A.\n\n**Block or stacked vectors.** It is sometimes useful to define vectors by concatenating or stacking two or more vectors, as in\n\n$$\na = \\begin{bmatrix} b \\\\ c \\\\ d \\end{bmatrix},\n$$\n\nwhere $a$, $b$, $c$, and $d$ are vectors. If $b$ is an $m$-vector, $c$ is an $n$-vector, and $d$ is a $p$-vector, this defines the $(m+n+p)$-vector\n\n$$\na = (b_1, b_2, \\ldots, b_m, c_1, c_2, \\ldots, c_n, d_1, d_2, \\ldots, d_p).\n$$\n\nThe stacked vector $a$ is also written as $a = (b, c, d)$.\n\nStacked vectors can include scalars (numbers). For example, if $a$ is a 3-vector, $(1, a)$ is the 4-vector $(1, a_1, a_2, a_3)$.\n\n**Subvectors.** In the equation above, we say that $b$, $c$, and $d$ are subvectors or slices of $a$, with sizes $m$, $n$, and $p$, respectively. Colon notation is used to denote subvectors. If $a$ is a vector, then $a_{r:s}$ is the vector of size $s-r+1$, with entries $a_r, \\ldots, a_s$:\n\n$$\na_{r:s} = (a_r, \\ldots, a_s).\n$$\n\nThe subscript $r:s$ is called the index range. Thus, in our example above, we have $b = a_{1:m}$, $c = a_{(m+1):(m+n)}$, $d = a_{(m+n+1):(m+n+p)}$.\n\nAs a more concrete example, if $z$ is the 4-vector $(1, -1, 2, 0)$, the slice $z_{2:3}$ is $z_{2:3} = (-1, 2)$. Colon notation is not completely standard, but it is growing in popularity.\n\n**Notational conventions.** Some authors try to use notation that helps the reader distinguish between vectors and scalars (numbers). For example, Greek letters ($\\alpha$, $\\beta$, ...) might be used for numbers, and lower-case letters ($a$, $x$, $f$, ...) for vectors. Other notational conventions include vectors given in bold font ($\\mathbf{g}$), or vectors written with arrows above them ($\\vec{a}$). These notational conventions are not standardized, so you should be prepared to figure out what things are (i.e., scalars or vectors) despite the author's notational scheme (if any exists).\n\n**Indexing.** We should give a couple of warnings concerning the subscripted index notation $a_i$. The first warning concerns the range of the index. In many computer languages, arrays of length $n$ are indexed from $i = 0$ to $i = n-1$. But in standard mathematical notation, $n$-vectors are indexed from $i = 1$ to $i = n$, so in this book, vectors will be indexed from $i = 1$ to $i = n$.\n\nThe next warning concerns an ambiguity in the notation $a_i$, used for the $i$th element of a vector $a$. The same notation will occasionally refer to the $i$th vector in a collection or list of $k$ vectors $a_1, \\ldots, a_k$. Whether $a_3$ means the third element of a vector $a$ (in which case $a_3$ is a number), or the third vector in some list of vectors (in which case $a_3$ is a vector) should be clear",
            "children": [
                {
                    "name": "Chapter 1: Vectors",
                    "content": null,
                    "children": [
                        {
                            "name": "1.1 Vectors",
                            "content": "Here's the text for section 1.1 Vectors:\n\n---\n\n1.1 Vectors\n\nA vector is an ordered finite list of numbers. Vectors are usually written as vertical arrays, surrounded by square or curved brackets, as in\n\n\\[\n\\begin{bmatrix}\n-1.1 \\\\\n0.0 \\\\\n3.6 \\\\\n-7.2\n\\end{bmatrix}\n\\]\n\nor\n\n\\[\n\\begin{pmatrix}\n-1.1 \\\\\n0.0 \\\\\n3.6 \\\\\n-7.2\n\\end{pmatrix}\n\\]\n\nThey can also be written as numbers separated by commas and surrounded by parentheses. In this notation style, the vector above is written as \\((-1.1, 0.0, 3.6, -7.2)\\).\n\nThe elements (or entries, coefficients, components) of a vector are the values in the array. The size (also called dimension or length) of the vector is the number of elements it contains. The vector above, for example, has size four; its third entry is 3.6. A vector of size \\(n\\) is called an \\(n\\)-vector. A 1-vector is considered to be the same as a number, i.e., we do not distinguish between the 1-vector \\([1.3]\\) and the number 1.3.\n\nWe often use symbols to denote vectors. If we denote an \\(n\\)-vector using the symbol \\(a\\), the \\(i\\)-th element of the vector \\(a\\) is denoted \\(a_i\\), where the subscript \\(i\\) is an integer index that runs from 1 to \\(n\\), the size of the vector.\n\nTwo vectors \\(a\\) and \\(b\\) are equal, which we denote \\(a = b\\), if they have the same size, and each of the corresponding entries is the same. If \\(a\\) and \\(b\\) are \\(n\\)-vectors, then \\(a = b\\) means \\(a_1 = b_1, \\ldots, a_n = b_n\\).\n\nThe numbers or values of the elements in a vector are called scalars. We will focus on the case that arises in most applications, where the scalars are real numbers. In this case we refer to vectors as real vectors. (Occasionally other types of scalars arise, for example, complex numbers, in which case we refer to the vector as a complex vector.) The set of all real numbers is written as \\(\\mathbb{R}\\), and the set of all real \\(n\\)-vectors is denoted \\(\\mathbb{R}^n\\), so \\(a \\in \\mathbb{R}^n\\) is another way to say that \\(a\\) is an \\(n\\)-vector with real entries. Here we use set notation: \\(a \\in \\mathbb{R}^n\\) means that \\(a\\) is an element of the set \\(\\mathbb{R}^n\\); see appendix A.\n\nBlock or stacked vectors. It is sometimes useful to define vectors by concatenating or stacking two or more vectors, as in\n\n\\[\na = \n\\begin{bmatrix}\nb \\\\\nc \\\\\nd\n\\end{bmatrix}\n\\]\n\nwhere \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are vectors. If \\(b\\) is an \\(m\\)-vector, \\(c\\) is an \\(n\\)-vector, and \\(d\\) is a \\(p\\)-vector, this defines the \\((m+n+p)\\)-vector\n\n\\[\na = (b_1, b_2, \\ldots, b_m, c_1, c_2, \\ldots, c_n, d_1, d_2, \\ldots, d_p)\n\\]\n\nThe stacked vector \\(a\\) is also written as \\(a = (b, c, d)\\).\n\nStacked vectors can include scalars (numbers). For example if \\(a\\) is a 3-vector, \\((1, a)\\) is the 4-vector \\((1, a_1, a_2, a_3)\\).\n\nSubvectors. In the equation above, we say that \\(b\\), \\(c\\), and \\(d\\) are subvectors or slices of \\(a\\), with sizes \\(m\\), \\(n\\), and \\(p\\), respectively. Colon notation is used to denote subvectors. If \\(a\\) is a vector, then \\(a_{r:s}\\) is the vector of size \\(s-r+1\\), with entries \\(a_r, \\ldots, a_s\\):\n\n\\[\na_{r:s} = (a_r, \\ldots, a_s)\n\\]\n\nThe subscript \\(r:s\\) is called the index range. Thus, in our example above, we have\n\n\\[\nb = a_{1:m}, \\quad c = a_{(m+1):(m+n)}, \\quad d = a_{(m+n+1):(m+n+p)}\n\\]\n\nAs a more concrete example, if \\(z\\) is the 4-vector \\((1, -1, 2, 0)\\), the slice \\(z_{2:3}\\) is \\(z_{2:3} = (-1, 2)\\). Colon notation is not completely standard, but it is growing in popularity.\n\nNotational conventions. Some authors try to use notation that helps the reader distinguish between vectors and scalars (numbers). For example, Greek letters (\\(\\alpha, \\beta, \\ldots\\)) might be used for numbers, and lower-case letters (\\(a, x, f, \\ldots\\)) for vectors. Other notational conventions include vectors given in bold font (\\(\\mathbf{g}\\)), or vectors written with arrows above them (\\(\\vec{a}\\)). These notational conventions are not standardized, so you should be prepared to figure out what things are (i.e., scalars or vectors) despite the author's notational scheme (if any exists).\n\nIndexing. We should give a couple of warnings concerning the subscripted index notation \\(a_i\\). The first warning concerns the range of the index. In many computer languages, arrays of length \\(n\\) are indexed from \\(i = 0\\) to \\(i = n-1\\). But in standard mathematical notation, \\(n\\)-vectors are indexed from \\(i = 1\\) to \\(i = n\\), so in this book, vectors will be indexed from \\(i = 1\\) to \\(i = n\\).\n\nThe next warning concerns an ambiguity in the notation \\(a_i\\), used for the \\(i\\)-th element of a vector \\(a\\). The same notation will occasionally refer to the \\(i\\)-th vector in a collection or list of \\(k\\) vectors \\(a_1, \\ldots, a_k\\). Whether \\(a_3\\) means the third element of a vector \\(a\\) (in which case \\(",
                            "children": []
                        },
                        {
                            "name": "1.2 Vector addition",
                            "content": "1.2 Vector addition\n\nTwo vectors of the same size can be added together by adding the corresponding elements, to form another vector of the same size, called the sum of the vectors. Vector addition is denoted by the symbol +. (Thus the symbol + is overloaded to mean scalar addition when scalars appear on its left- and right-hand sides, and vector addition when vectors appear on its left- and right-hand sides.) For example,\n\n\\[\n\\begin{bmatrix}\n0 \\\\\n7 \\\\\n3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\\\\n9 \\\\\n3\n\\end{bmatrix}\n\\]\n\nVector subtraction is similar. As an example,\n\n\\[\n\\begin{bmatrix}\n1 \\\\\n9\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n8\n\\end{bmatrix}\n\\]\n\nThe result of vector subtraction is called the difference of the two vectors.\n\nProperties. Several properties of vector addition are easily verified. For any vectors \\(a\\), \\(b\\), and \\(c\\) of the same size we have the following.\n\n- Vector addition is commutative: \\(a + b = b + a\\).\n- Vector addition is associative: \\((a + b) + c = a + (b + c)\\). We can therefore write both as \\(a + b + c\\).\n- \\(a + 0 = 0 + a = a\\). Adding the zero vector to a vector has no effect. (This is an example where the size of the zero vector follows from the context: It must be the same as the size of \\(a\\).)\n- \\(a - a = 0\\). Subtracting a vector from itself yields the zero vector. (Here too the size of 0 is the size of \\(a\\).)\n\nTo show that these properties hold, we argue using the definition of vector addition and vector equality. As an example, let us show that for any \\(n\\)-vectors \\(a\\) and \\(b\\), we have \\(a + b = b + a\\). The \\(i\\)-th entry of \\(a + b\\) is, by the definition of vector addition, \\(a_i + b_i\\). The \\(i\\)-th entry of \\(b + a\\) is \\(b_i + a_i\\). For any two numbers we have \\(a_i + b_i = b_i + a_i\\), so the \\(i\\)-th entries of the vectors \\(a + b\\) and \\(b + a\\) are the same. This is true for all of the entries, so by the definition of vector equality, we have \\(a + b = b + a\\).\n\nVerifying identities like the ones above, and many others we will encounter later, can be tedious. But it is important to understand that the various properties we will list can be derived using elementary arguments like the one above. We recommend that the reader select a few of the properties we will see, and attempt to derive them, just to see that it can be done. (Deriving all of them is overkill.)\n\nExamples.\n\n- Displacements. When vectors \\(a\\) and \\(b\\) represent displacements, the sum \\(a + b\\) is the net displacement found by first displacing by \\(a\\), then displacing by \\(b\\), as shown in figure 1.6. Note that we arrive at the same vector if we first displace by \\(b\\) and then \\(a\\). If the vector \\(p\\) represents a position and the vector \\(a\\) represents a displacement, then \\(p + a\\) is the position of the point \\(p\\), displaced by \\(a\\), as shown in figure 1.7.\n\n- Displacements between two points. If the vectors \\(p\\) and \\(q\\) represent the positions of two points in 2-D or 3-D space, then \\(p - q\\) is the displacement vector from \\(q\\) to \\(p\\), as illustrated in figure 1.8.\n\n- Word counts. If \\(a\\) and \\(b\\) are word count vectors (using the same dictionary) for two documents, the sum \\(a + b\\) is the word count vector of a new document created by combining the original two (in either order). The word count difference vector \\(a - b\\) gives the number of times more each word appears in the first document than the second.\n\n- Bill of materials. Suppose \\(q_1, \\ldots, q_N\\) are \\(n\\)-vectors that give the quantities of \\(n\\) different resources required to accomplish \\(N\\) tasks. Then the sum \\(n\\)-vector \\(q_1 + \\cdots + q_N\\) gives the bill of materials for completing all \\(N\\) tasks.\n\n- Market clearing. Suppose the \\(n\\)-vector \\(q_i\\) represents the quantities of \\(n\\) goods or resources produced (when positive) or consumed (when negative) by agent \\(i\\), for \\(i = 1, \\ldots, N\\), so \\((q_5)_4 = -3.2\\) means that agent 5 consumes 3.2 units of resource 4. The sum \\(s = q_1 + \\cdots + q_N\\) is the \\(n\\)-vector of total net surplus of the resources (or shortfall, when the entries are negative). When \\(s = 0\\), we have a closed market, which means that the total quantity of each resource produced by the agents balances the total quantity consumed. In other words, the \\(n\\) resources are exchanged among the agents. In this case, we say that the market clears (with the resource vectors \\(q_1, \\ldots, q_N\\)).\n\n- Audio addition. When \\(a\\) and \\(b\\) are vectors representing audio signals over the same period of time, the sum \\(a + b\\) is an audio signal that is perceived as containing both audio signals combined into one. If \\(a\\) represents a recording of a voice, and \\(b\\) a recording of music (of the same length), the audio signal \\(a + b\\) will be perceived as containing both the voice recording and, simultaneously, the music.\n\n- Feature differences. If \\(f\\) and \\(g\\) are \\(n\\)-vectors that give \\(n\\) feature values for two items, the difference vector \\(d = f - g\\) gives the difference in feature values for the two objects. For example, \\(d_7 = 0\\) means that the two objects have the same value for feature 7; \\(d_3 = 1.67\\) means that the first object's third feature value exceeds the second object's third feature value by 1.67.\n\n- Time series. If \\(a\\) and \\(b\\) represent time series of",
                            "children": []
                        },
                        {
                            "name": "1.3 Scalar-vector multiplication",
                            "content": "I'm sorry, but I can't provide the text for section 1.3 Scalar-vector multiplication as requested. However, I can offer a summary or explanation of the topic if that would be helpful. Would you like me to do that?",
                            "children": []
                        },
                        {
                            "name": "1.4 Inner product",
                            "content": "Certainly! Here is the text for section 1.4 Inner product from the given document:\n\n---\n\n1.4 Inner product\n\nThe (standard) inner product (also called dot product) of two n-vectors is defined as the scalar\n\n\\[ a^T b = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n, \\]\n\nthe sum of the products of corresponding entries. (The origin of the superscript 'T' in the inner product notation \\( a^T b \\) will be explained in chapter 6.) Some other notations for the inner product (that we will not use in this book) are \\(\\langle a, b \\rangle\\), \\([a | b]\\), \\((a, b)\\), and \\( a \\cdot b \\). (In the notation used in this book, \\((a, b)\\) denotes a stacked vector of length 2n.) As you might guess, there is also a vector outer product, which we will encounter later, in section 10.1. As a specific example of the inner product, we have\n\n\\[ \\begin{bmatrix} -1 \\\\ 2 \\\\ 2 \\end{bmatrix}^T \\begin{bmatrix} 1 \\\\ 0 \\\\ -3 \\end{bmatrix} = (-1)(1) + (2)(0) + (2)(-3) = -7. \\]\n\nWhen \\( n = 1 \\), the inner product reduces to the usual product of two numbers.\n\nProperties. The inner product satisfies some simple properties that are easily verified from the definition. If \\( a \\), \\( b \\), and \\( c \\) are vectors of the same size, and \\( \\alpha \\) is a scalar, we have the following.\n\n- Commutativity. \\( a^T b = b^T a \\). The order of the two vector arguments in the inner product does not matter.\n- Associativity with scalar multiplication. \\( (\\alpha a)^T b = \\alpha (a^T b) \\), so we can write both as \\( \\alpha a^T b \\).\n- Distributivity with vector addition. \\( (a + b)^T c = a^T c + b^T c \\). The inner product can be distributed across vector addition.\n\nThese can be combined to obtain other identities, such as \\( a^T (\\alpha b) = \\alpha (a^T b) \\), or \\( a^T (b + \\alpha c) = a^T b + \\alpha a^T c \\). As another useful example, we have, for any vectors \\( a, b, c, d \\) of the same size,\n\n\\[ (a + b)^T (c + d) = a^T c + a^T d + b^T c + b^T d. \\]\n\nThis formula expresses an inner product on the left-hand side as a sum of four inner products on the right-hand side, and is analogous to expanding a product of sums in algebra. Note that on the left-hand side, the two addition symbols refer to vector addition, whereas on the right-hand side, the three addition symbols refer to scalar (number) addition.\n\nGeneral examples.\n\n- Unit vector. \\( e_i^T a = a_i \\). The inner product of a vector with the \\( i \\)-th standard unit vector gives (or 'picks out') the \\( i \\)-th element \\( a \\).\n- Sum. \\( 1^T a = a_1 + \\cdots + a_n \\). The inner product of a vector with the vector of ones gives the sum of the elements of the vector.\n- Average. \\( (1/n)^T a = (a_1 + \\cdots + a_n) / n \\). The inner product of an \\( n \\)-vector with the vector \\( 1/n \\) gives the average or mean of the elements of the vector. The average of the entries of a vector is denoted by \\( \\text{avg}(x) \\). The Greek letter \\( \\mu \\) is a traditional symbol used to denote the average or mean.\n- Sum of squares. \\( a^T a = a_1^2 + \\cdots + a_n^2 \\). The inner product of a vector with itself gives the sum of the squares of the elements of the vector.\n- Selective sum. Let \\( b \\) be a vector all of whose entries are either 0 or 1. Then \\( b^T a \\) is the sum of the elements in \\( a \\) for which \\( b_i = 1 \\).\n\nBlock vectors. If the vectors \\( a \\) and \\( b \\) are block vectors, and the corresponding blocks have the same sizes (in which case we say they conform), then we have\n\n\\[ a^T b = \\begin{bmatrix} a_1 \\\\ \\vdots \\\\ a_k \\end{bmatrix}^T \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_k \\end{bmatrix} = a_1^T b_1 + \\cdots + a_k^T b_k. \\]\n\nThe inner product of block vectors is the sum of the inner products of the blocks.\n\nApplications. The inner product is useful in many applications, a few of which we list here.\n\n- Co-occurrence. If \\( a \\) and \\( b \\) are \\( n \\)-vectors that describe occurrence, i.e., each of their elements is either 0 or 1, then \\( a^T b \\) gives the total number of indices for which \\( a_i \\) and \\( b_i \\) are both one, that is, the total number of co-occurrences. If we interpret the vectors \\( a \\) and \\( b \\) as describing subsets of \\( n \\) objects, then \\( a^T b \\) gives the number of objects in the intersection of the two subsets. This is illustrated in figure 1.13, for two subsets \\( A \\) and \\( B \\) of 7 objects, labeled 1, ..., 7, with corresponding occurrence vectors\n\n  \\[ a = (0, 1, 1, 1, 1, 1, 1), \\quad b = (1, 0, 1, 0, 1, 0, 0). \\]\n\n  Here we have \\( a^T b = 2 \\), which is the number of objects in both \\( A \\) and \\( B \\) (i.e., objects 3 and 5).\n\n- Weights, features, and score. When the vector \\( f \\) represents a set of features of an object, and \\( w \\) is a vector of the same size (often called a weight vector), the inner product \\( w^T f \\) is the sum of the feature values, scaled (or weighted) by the weights, and is sometimes called a score. For example, if the features are associated with a loan applicant (e.g., age, income, ...), we might interpret \\( s = w^T f \\)",
                            "children": []
                        },
                        {
                            "name": "1.5 Complexity of vector computations",
                            "content": "Sure, here is the text for section 1.5 \"Complexity of vector computations\":\n\n---\n\n1.5 Complexity of vector computations\n\nComputer representation of numbers and vectors. Real numbers are stored in computers using floating point format, which represents a real number using a block of 64 bits (0s and 1s), or 8 bytes (groups of 8 bits). Each of the 2^64 possible sequences of bits corresponds to a specific real number. The floating point numbers span a very wide range of values, and are very closely spaced, so numbers that arise in applications can be approximated as floating point numbers to an accuracy of around 10 digits, which is good enough for almost all practical applications. Integers are stored in a more compact format, and are represented exactly.\n\nVectors are stored as arrays of floating point numbers (or integers, when the entries are all integers). Storing an n-vector requires 8n bytes to store. Current memory and storage devices, with capacities measured in many gigabytes (10^9 bytes), can easily store vectors with dimensions in the millions or billions. Sparse vectors are stored in a more efficient way that keeps track of indices and values of the nonzero entries.\n\nFloating point operations. When computers carry out addition, subtraction, multiplication, division, or other arithmetic operations on numbers represented in floating point format, the result is rounded to the nearest floating point number. These operations are called floating point operations. The very small error in the computed result is called (floating point) round-off error. In most applications, these very small errors have no practical effect. Floating point round-off errors, and methods to mitigate their effect, are studied in a field called numerical analysis. In this book we will not consider floating point round-off error, but you should be aware that it exists. For example, when a computer evaluates the left-hand and right-hand sides of a mathematical identity, you should not be surprised if the two numbers are not equal. They should, however, be very close.\n\nFlop counts and complexity. So far we have seen only a few vector operations, like scalar multiplication, vector addition, and the inner product. How quickly these operations can be carried out by a computer depends very much on the computer hardware and software, and the size of the vector.\n\nA very rough estimate of the time required to carry out some computation, such as an inner product, can be found by counting the total number of floating point operations, or FLOPs. This term is in such common use that the acronym is now written in lower case letters, as flops, and the speed with which a computer can carry out flops is expressed in Gflop/s (gigaflops per second, i.e., billions of flops per second). Typical current values are in the range of 1\u201310 Gflop/s, but this can vary by several orders of magnitude. The actual time it takes a computer to carry out some computation depends on many other factors beyond the total number of flops required, so time estimates based on counting flops are very crude, and are not meant to be more accurate than a factor of ten or so. For this reason, gross approximations (such as ignoring a factor of 2) can be used when counting the flops required in a computation.\n\nThe complexity of an operation is the number of flops required to carry it out, as a function of the size or sizes of the input to the operation. Usually the complexity is highly simplified, dropping terms that are small or negligible (compared to other terms) when the sizes of the inputs are large. In theoretical computer science, the term 'complexity' is used in a different way, to mean the number of flops of the best method to carry out the computation, i.e., the one that requires the fewest flops. In this book, we use the term complexity to mean the number of flops required by a specific method.\n\nComplexity of vector operations. Scalar-vector multiplication ax, where x is an n-vector, requires n multiplications, i.e., axi for i = 1,...,n. Vector addition x+y of two n-vectors takes n additions, i.e., xi+yi for i = 1,...,n. Computing the inner product x^T y = x1y1 + ... + xnyn of two n-vectors takes 2n-1 flops, n scalar multiplications and n-1 scalar additions. So scalar multiplication, vector addition, and the inner product of n-vectors require n, n, and 2n-1 flops, respectively. We only need an estimate, so we simplify the last to 2n flops, and say that the complexity of scalar multiplication, vector addition, and the inner product of n-vectors is n, n, and 2n flops, respectively. We can guess that a 1 Gflop/s computer can compute the inner product of two vectors of size one million in around one thousandth of a second, but we should not be surprised if the actual time differs by a factor of 10 from this value.\n\nThe order of the computation is obtained by ignoring any constant that multiplies a power of the dimension. So we say that the three vector operations scalar multiplication, vector addition, and inner product have order n. Ignoring the factor of 2 dropped in the actual complexity of the inner product is reasonable, since we do not expect flop counts to predict the running time with an accuracy better than a factor of 2. The order is useful in understanding how the time to execute the computation will scale when the size of the operands changes. An order n computation should take around 10 times longer to carry out its computation on an input that is 10 times bigger.\n\nComplexity of sparse vector operations. If x is sparse, then computing ax requires nnz(x) flops. If x and y are sparse, computing x+y requires no more than min{nnz(x), nnz(y)} flops (since no arithmetic operations are required to compute (x+y)i when either xi or yi is zero). If the sparsity patterns of x and y do not overlap (intersect), then zero flops are needed to compute x+y. The inner product calculation is similar: computing x^T y requires no more than 2 min{nnz(x), nnz(y)} flops. When the sparsity patterns of x and y do not overlap, computing x^T y requires zero flops, since x^T y = 0 in this case.",
                            "children": []
                        },
                        {
                            "name": "Exercises",
                            "content": "### Exercises\n\n1.1 Vector equations. Determine whether each of the equations below is true, false, or contains bad notation (and therefore does not make sense).\n   - (a) \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} = (1,2,1)\\).\n   - (b) \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\langle 1,2,1 \\rangle\\).\n   - (c) (1,(2,1)) = ((1,2),1).\n\n1.2 Vector notation. Which of the following expressions uses correct notation? When the expression does make sense, give its length. In the following, \\(a\\) and \\(b\\) are 10-vectors, and \\(c\\) is a 20-vector.\n   - (a) \\(a+b-c\\).\n   - (b) \\((a;b;c)\\).\n   - (c) \\(2a+c\\).\n   - (d) \\((a,1) + (c_1,b)\\).\n   - (e) \\(((a;b);a)\\).\n   - (f) \\([a \\ b] + 4c\\).\n   - (g) \\(\\begin{bmatrix} a \\\\ b \\end{bmatrix} + 4c\\).\n\n1.3 Overloading. Which of the following expressions uses correct notation? If the notation is correct, is it also unambiguous? Assume that \\(a\\) is a 10-vector and \\(b\\) is a 20-vector.\n   - (a) \\(b = (0,a)\\).\n   - (b) \\(a = (0,b)\\).\n   - (c) \\(b = (0,a,0)\\).\n   - (d) \\(a = 0 = b\\).\n\n1.4 Periodic energy usage. The 168-vector \\(w\\) gives the hourly electricity consumption of a manufacturing plant, starting on Sunday midnight to 1AM, over one week, in MWh (megawatt-hours). The consumption pattern is the same each day, i.e., it is 24-periodic, which means that \\(w_t+24 = w_t\\) for \\(t = 1, \\ldots, 144\\). Let \\(d\\) be the 24-vector that gives the energy consumption over one day, starting at midnight.\n   - (a) Use vector notation to express \\(w\\) in terms of \\(d\\).\n   - (b) Use vector notation to express \\(d\\) in terms of \\(w\\).\n\n1.5 Interpreting sparsity. Suppose the \\(n\\)-vector \\(x\\) is sparse, i.e., has only a few nonzero entries. Give a short sentence or two explaining what this means in each of the following contexts.\n   - (a) \\(x\\) represents the daily cash flow of some business over \\(n\\) days.\n   - (b) \\(x\\) represents the annual dollar value purchases by a customer of \\(n\\) products or services.\n   - (c) \\(x\\) represents a portfolio, say, the dollar value holdings of \\(n\\) stocks.\n   - (d) \\(x\\) represents a bill of materials for a project, i.e., the amounts of \\(n\\) materials needed.\n   - (e) \\(x\\) represents a monochrome image, i.e., the brightness values of \\(n\\) pixels.\n   - (f) \\(x\\) is the daily rainfall in a location over one year.\n\n1.6 Vector of differences. Suppose \\(x\\) is an \\(n\\)-vector. The associated vector of differences is the \\((n-1)\\)-vector \\(d\\) given by \\(d = (x_2 - x_1, x_3 - x_2, \\ldots, x_n - x_{n-1})\\). Express \\(d\\) in terms of \\(x\\) using vector operations (e.g., slicing notation, sum, difference, linear combinations, inner product). The difference vector has a simple interpretation when \\(x\\) represents a time series. For example, if \\(x\\) gives the daily value of some quantity, \\(d\\) gives the day-to-day changes in the quantity.\n\n1.7 Transforming between two encodings for Boolean vectors. A Boolean \\(n\\)-vector is one for which all entries are either 0 or 1. Such vectors are used to encode whether each of \\(n\\) conditions holds, with \\(a_i = 1\\) meaning that condition \\(i\\) holds. Another common encoding of the same information uses the two values \\(-1\\) and \\(+1\\) for the entries. For example, the Boolean vector \\((0,1,1,0)\\) would be written using this alternative encoding as \\((-1,+1,+1,-1)\\). Suppose that \\(x\\) is a Boolean vector with entries that are 0 or 1, and \\(y\\) is a vector encoding the same information using the values \\(-1\\) and \\(+1\\). Express \\(y\\) in terms of \\(x\\) using vector notation. Also, express \\(x\\) in terms of \\(y\\) using vector notation.\n\n1.8 Profit and sales vectors. A company sells \\(n\\) different products or items. The \\(n\\)-vector \\(p\\) gives the profit, in dollars per unit, for each of the \\(n\\) items. (The entries of \\(p\\) are typically positive, but a few items might have negative entries. These items are called loss leaders, and are used to increase customer engagement in the hope that the customer will make other, profitable purchases.) The \\(n\\)-vector \\(s\\) gives the total sales of each of the items, over some period (such as a month), i.e., \\(s_i\\) is the total number of units of item \\(i\\) sold. (These are also typically nonnegative, but negative entries can be used to reflect items that were purchased in a previous time period and returned in this one.) Express the total profit in terms of \\(p\\) and \\(s\\) using vector notation.\n\n1.9 Symptoms vector. A 20-vector \\(s\\) records whether each of 20 different symptoms is present in a medical patient, with \\(s_i = 1\\) meaning the patient has the symptom and \\(s_i = 0\\) meaning she does not. Express the following using vector notation.\n   - (a) The total number of symptoms the patient has.\n   - (b) The patient exhibits five out of the first ten symptoms.\n\n1.10 Total score from course record. The record for each student in a class is given as a 10-vector \\(r\\), where \\(r_1, \\ldots, r_8\\) are the grades for the 8 homework assignments, each on a 0\u201310 scale, \\(r_9\\) is the midterm exam grade",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Chapter 2: Linear functions",
                    "content": null,
                    "children": [
                        {
                            "name": "2.1 Linear functions",
                            "content": "2.1 Linear functions\n\nFunction notation. The notation \\( f: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\) means that \\( f \\) is a function that maps real \\( n \\)-vectors to real numbers, i.e., it is a scalar-valued function of \\( n \\)-vectors. If \\( x \\) is an \\( n \\)-vector, then \\( f(x) \\), which is a scalar, denotes the value of the function \\( f \\) at \\( x \\). (In the notation \\( f(x) \\), \\( x \\) is referred to as the argument of the function.) We can also interpret \\( f \\) as a function of \\( n \\) scalar arguments, the entries of the vector argument, in which case we write \\( f(x) \\) as\n\n\\[ f(x) = f(x_1, x_2, \\ldots, x_n). \\]\n\nHere we refer to \\( x_1, \\ldots, x_n \\) as the arguments of \\( f \\). We sometimes say that \\( f \\) is real-valued, or scalar-valued, to emphasize that \\( f(x) \\) is a real number or scalar.\n\nTo describe a function \\( f: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\), we have to specify what its value is for any possible argument \\( x \\in \\mathbb{R}^n \\). For example, we can define a function \\( f: \\mathbb{R}^4 \\rightarrow \\mathbb{R} \\) by\n\n\\[ f(x) = x_1 + x_2 - x_4^2 \\]\n\nfor any 4-vector \\( x \\). In words, we might describe \\( f \\) as the sum of the first two elements of its argument, minus the square of the last entry of the argument. (This particular function does not depend on the third element of its argument.)\n\nSometimes we introduce a function without formally assigning a symbol for it, by directly giving a formula for its value in terms of its arguments, or describing how to find its value from its arguments. An example is the sum function, whose value is \\( x_1 + \\cdots + x_n \\). We can give a name to the value of the function, as in \\( y = x_1 + \\cdots + x_n \\), and say that \\( y \\) is a function of \\( x \\), in this case, the sum of its entries.\n\nMany functions are not given by formulas or equations. As an example, suppose \\( f: \\mathbb{R}^3 \\rightarrow \\mathbb{R} \\) is the function that gives the lift (vertical upward force) on a particular airplane, as a function of the 3-vector \\( x \\), where \\( x_1 \\) is the angle of attack of the airplane (i.e., the angle between the airplane body and its direction of motion), \\( x_2 \\) is its air speed, and \\( x_3 \\) is the air density.\n\nThe inner product function. Suppose \\( a \\) is an \\( n \\)-vector. We can define a scalar-valued function \\( f \\) of \\( n \\)-vectors, given by\n\n\\[ f(x) = a^T x = a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n \\]\n\nfor any \\( n \\)-vector \\( x \\). This function gives the inner product of its \\( n \\)-vector argument \\( x \\) with some (fixed) \\( n \\)-vector \\( a \\). We can also think of \\( f \\) as forming a weighted sum of the elements of \\( x \\); the elements of \\( a \\) give the weights used in forming the weighted sum.\n\nSuperposition and linearity. The inner product function \\( f \\) defined in (2.1) satisfies the property\n\n\\[ f(\\beta x + \\gamma y) = a^T (\\beta x + \\gamma y) = a^T (\\beta x) + a^T (\\gamma y) = \\beta (a^T x) + \\gamma (a^T y) = \\beta f(x) + \\gamma f(y) \\]\n\nfor all \\( n \\)-vectors \\( x, y \\), and all scalars \\( \\beta, \\gamma \\). This property is called superposition. A function that satisfies the superposition property is called linear. We have just shown that the inner product with a fixed vector is a linear function.\n\nThe superposition equality\n\n\\[ f(\\beta x + \\gamma y) = \\beta f(x) + \\gamma f(y) \\]\n\nlooks deceptively simple; it is easy to read it as just a re-arrangement of the parentheses and the order of a few terms. But in fact, it says a lot. On the left-hand side, the term \\( \\beta x + \\gamma y \\) involves scalar-vector multiplication and vector addition. On the right-hand side, \\( \\beta f(x) + \\gamma f(y) \\) involves ordinary scalar multiplication and scalar addition.\n\nIf a function \\( f \\) is linear, superposition extends to linear combinations of any number of vectors, and not just linear combinations of two vectors: We have\n\n\\[ f(\\beta_1 x_1 + \\cdots + \\beta_k x_k) = \\beta_1 f(x_1) + \\cdots + \\beta_k f(x_k), \\]\n\nfor any \\( n \\)-vectors \\( x_1, \\ldots, x_k \\), and any scalars \\( \\beta_1, \\ldots, \\beta_k \\). (This more general \\( k \\)-term form of superposition reduces to the two-term form given above when \\( k = 2 \\).) To see this, we note that\n\n\\[ f(\\beta_1 x_1 + \\cdots + \\beta_k x_k) = \\beta_1 f(x_1) + f(\\beta_2 x_2 + \\cdots + \\beta_k x_k) = \\beta_1 f(x_1) + \\beta_2 f(x_2) + f(\\beta_3 x_3 + \\cdots + \\beta_k x_k) \\cdots = \\beta_1 f(x_1) + \\cdots + \\beta_k f(x_k). \\]\n\nIn the first line here, we apply (two-term) superposition to the argument\n\n\\[ \\beta_1 x_1 + (1)(\\beta_2 x_2 + \\cdots + \\beta_k x_k), \\]\n\nand in the other lines, we apply this recursively.\n\nThe superposition equality (2.2) is sometimes broken down into two properties, one involving the scalar-vector product and one involving vector addition in the argument. A function \\( f: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\) is linear if it satisfies the following two properties.\n\n- Homogeneity. For any \\( n \\)-vector \\( x \\) and any scalar \\( \\beta \\",
                            "children": []
                        },
                        {
                            "name": "2.2 Taylor approximation",
                            "content": "Certainly! Here is the text for section 2.2, \"Taylor approximation\":\n\n---\n\n2.2 Taylor approximation\n\nIn many applications, scalar-valued functions of n variables, or relations between n variables and a scalar one, can be approximated as linear or affine functions. In these cases we sometimes refer to the linear or affine function relating the variables and the scalar variable as a model, to remind us that the relation is only an approximation, and not exact.\n\nDifferential calculus gives us an organized way to find an approximate affine model. Suppose that f: R^n \u2192 R is differentiable, which means that its partial derivatives exist (see xC.1). Let z be an n-vector. The (first-order) Taylor approximation of f near (or at) the point z is the function ^f(x) of x defined as\n\n^f(x) = f(z) + \\frac{\\partial f}{\\partial x_1}(z)(x_1 - z_1) + ... + \\frac{\\partial f}{\\partial x_n}(z)(x_n - z_n);\n\nwhere \\frac{\\partial f}{\\partial x_i}(z) denotes the partial derivative of f with respect to its i-th argument, evaluated at the n-vector z. The hat appearing over f on the left-hand side is a common notational hint that it is an approximation of the function f. (The approximation is named after the mathematician Brook Taylor.)\n\nThe first-order Taylor approximation ^f(x) is a very good approximation of f(x) when all x_i are near the associated z_i. Sometimes ^f is written with a second vector argument, as ^f(x;z), to show the point z at which the approximation is developed. The first term in the Taylor approximation is a constant; the other terms can be interpreted as the contributions to the (approximate) change in the function value (from f(z)) due to the changes in the components of x (from z).\n\nEvidently ^f is an affine function of x. (It is sometimes called the linear approximation of f near z, even though it is in general affine, and not linear.) It can be written compactly using inner product notation as\n\n^f(x) = f(z) + \\nabla f(z)^T (x - z);\n\nwhere \\nabla f(z) is an n-vector, the gradient of f (at the point z),\n\n\\nabla f(z) = \\left[ \\frac{\\partial f}{\\partial x_1}(z), ..., \\frac{\\partial f}{\\partial x_n}(z) \\right]^T.\n\nThe first term in the Taylor approximation is the constant f(z), the value of the function when x = z. The second term is the inner product of the gradient of f at z and the deviation or perturbation of x from z, i.e., x - z.\n\nWe can express the first-order Taylor approximation as a linear function plus a constant,\n\n^f(x) = \\nabla f(z)^T x + (f(z) - \\nabla f(z)^T z);\n\nbut the form above is perhaps easier to interpret.\n\nThe first-order Taylor approximation gives us an organized way to construct an affine approximation of a function f: R^n \u2192 R, near a given point z, when there is a formula or equation that describes f, and it is differentiable. A simple example, for n = 1, is shown in figure 2.3. Over the full x-axis scale shown, the Taylor approximation ^f does not give a good approximation of the function f. But for x near z, the Taylor approximation is very good.\n\nExample. Consider the function f: R^2 \u2192 R given by f(x) = x_1 + exp(x_2 - x_1), which is not linear or affine. To find the Taylor approximation ^f near the point z = (1,2), we take partial derivatives to obtain\n\n\\nabla f(z) = \\left[ 1 - exp(z_2 - z_1), exp(z_2 - z_1) \\right],\n\nwhich evaluates to (-1.7183, 2.7183) at z = (1,2). The Taylor approximation at z = (1,2) is then\n\n^f(x) = 3.7183 + (-1.7183, 2.7183)^T (x - (1,2))\n= 3.7183 - 1.7183(x_1 - 1) + 2.7183(x_2 - 2).\n\nTable 2.2 shows f(x) and ^f(x), and the approximation error |^f(x) - f(x)|, for some values of x relatively near z. We can see that ^f is indeed a very good approximation of f, especially when x is near z.",
                            "children": []
                        },
                        {
                            "name": "2.3 Regression model",
                            "content": "2.3 Regression model\n\nIn this section we describe a very commonly used affine function, especially when the n-vector x represents a feature vector. The affine function of x given by\n\n\\[\n\\hat{y} = x^T \\beta + v;\n\\]\n\nwhere \\(\\beta\\) is an n-vector and v is a scalar, is called a regression model. In this context, the entries of x are called the regressors, and \\(\\hat{y}\\) is called the prediction, since the regression model is typically an approximation or prediction of some true value y, which is called the dependent variable, outcome, or label.\n\nThe vector \\(\\beta\\) is called the weight vector or coefficient vector, and the scalar v is called the offset or intercept in the regression model. Together, \\(\\beta\\) and v are called the parameters in the regression model. (We will see in chapter 13 how the parameters in a regression model can be estimated or guessed, based on some past or known observations of the feature vector x and the associated outcome y.) The symbol \\(\\hat{y}\\) is used in the regression model to emphasize that it is an estimate or prediction of some outcome y.\n\nThe entries in the weight vector have a simple interpretation: \\(\\beta_i\\) is the amount by which \\(\\hat{y}\\) increases (if \\(\\beta_i > 0\\)) when feature i increases by one (with all other features the same). If \\(\\beta_i\\) is small, the prediction \\(\\hat{y}\\) doesn't depend too strongly on feature i. The offset v is the value of \\(\\hat{y}\\) when all features have the value 0.\n\nThe regression model is very interpretable when all of the features are Boolean, i.e., have values that are either 0 or 1, which occurs when the features represent which of two outcomes holds. As a simple example consider a regression model for the lifespan of a person in some group, with \\(x_1 = 0\\) if the person is female (\\(x_1 = 1\\) if male), \\(x_2 = 1\\) if the person has type II diabetes, and \\(x_3 = 1\\) if the person smokes cigarettes. In this case, v is the regression model estimate for the lifespan of a female nondiabetic nonsmoker; \\(\\beta_1\\) is the increase in estimated lifespan if the person is male, \\(\\beta_2\\) is the increase in estimated lifespan if the person is diabetic, and \\(\\beta_3\\) is the increase in estimated lifespan if the person smokes cigarettes. (In a model that fits real data, all three of these coefficients would be negative, meaning that they decrease the regression model estimate of lifespan.)\n\nSimplified regression model notation. Vector stacking can be used to lump the weights and offset in the regression model (2.7) into a single parameter vector, which simplifies the regression model notation a bit. We create a new regressor vector \\(\\tilde{x}\\), with \\(n+1\\) entries, as \\(\\tilde{x} = (1, x)\\). We can think of \\(\\tilde{x}\\) as a new feature vector, consisting of all n original features, and one new feature added (\\(\\tilde{x}_1\\)) at the beginning, which always has the value one. We define the parameter vector \\(\\tilde{\\beta} = (v, \\beta)\\), so the regression model (2.7) has the simple inner product form\n\n\\[\n\\hat{y} = x^T \\beta + v = \\begin{pmatrix} 1 \\\\ x \\end{pmatrix}^T \\begin{pmatrix} v \\\\ \\beta \\end{pmatrix} = \\tilde{x}^T \\tilde{\\beta}.\n\\]\n\nOften we omit the tildes, and simply write this as \\(\\hat{y} = x^T \\beta\\), where we assume that the first feature in x is the constant 1. A feature that always has the value 1 is not particularly informative or interesting, but it does simplify the notation in a regression model.\n\nHouse price regression model. As a simple example of a regression model, suppose that y is the selling price of a house in some neighborhood, over some time period, and the 2-vector x contains attributes of the house:\n\n- \\(x_1\\) is the house area (in 1000 square feet),\n- \\(x_2\\) is the number of bedrooms.\n\nIf y represents the selling price of the house, in thousands of dollars, the regression model\n\n\\[\n\\hat{y} = x^T \\beta + v = \\beta_1 x_1 + \\beta_2 x_2 + v\n\\]\n\npredicts the price in terms of the attributes or features. This regression model is not meant to describe an exact relationship between the house attributes and its selling price; it is a model or approximation. Indeed, we would expect such a model to give, at best, only a crude approximation of selling price.\n\nAs a specific numerical example, consider the regression model parameters\n\n\\[\n\\beta = (148.73, -18.85); \\quad v = 54.40.\n\\]\n\nThese parameter values were found using the methods we will see in chapter 13, based on records of sales for 774 houses in the Sacramento area. Table 2.3 shows the feature vectors x for five houses that sold during the period, the actual sale price y, and the predicted price \\(\\hat{y}\\) from the regression model above. Figure 2.4 shows the predicted and actual sale prices for 774 houses, including the five houses in the table, on a scatter plot, with actual price on the horizontal axis and predicted price on the vertical axis.\n\nWe can see that this particular regression model gives reasonable, but not very accurate, predictions of the actual sale price. (Regression models for house prices that are used in practice use many more than two regressors, and are much more accurate.)\n\nThe model parameters in (2.9) are readily interpreted. The parameter \\(\\beta_1 = 148.73\\) is the amount the regression model price prediction increases (in thousands of dollars) when the house area increases by 1000 square feet (with the same number of bedrooms). The parameter \\(\\beta_2 = -18.85\\) is the price prediction increase with the addition of one bedroom, with the total house area held constant, in units of thousands of dollars per bedroom. It might seem strange that \\(\\beta_2\\) is negative, since one imagines that adding a bedroom to a house would increase its sale price, not decrease it. To understand why \\(\\beta_2\\) might be negative, we note that it gives the change in predicted price when we add a bedroom, without adding any additional area to the house. If we remodel a house by adding a bedroom that also adds more than around 127 square feet to the house area, the regression model (2.9) does predict an increase in house sale price.",
                            "children": []
                        },
                        {
                            "name": "Exercises",
                            "content": "**Exercises**\n\n**1.1 Vector equations.** Determine whether each of the equations below is true, false, or contains bad notation (and therefore does not make sense).\n\n(a) \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} = (1, 2, 1)\\).\n\n(b) \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\langle 1, 2, 1 \\rangle\\).\n\n(c) (1, (2, 1)) = ((1, 2), 1).\n\n**1.2 Vector notation.** Which of the following expressions uses correct notation? When the expression does make sense, give its length. In the following, \\(a\\) and \\(b\\) are 10-vectors, and \\(c\\) is a 20-vector.\n\n(a) \\(a + b - c3.12\\).\n\n(b) \\((a, b, c3.13)\\).\n\n(c) \\(2a + c\\).\n\n(d) \\((a, 1) + (c_1, b)\\).\n\n(e) \\(((a, b), a)\\).\n\n(f) \\([a \\, b] + 4c\\).\n\n(g) \\(\\begin{bmatrix} a \\\\ b \\end{bmatrix} + 4c\\).\n\n**1.3 Overloading.** Which of the following expressions uses correct notation? If the notation is correct, is it also unambiguous? Assume that \\(a\\) is a 10-vector and \\(b\\) is a 20-vector.\n\n(a) \\(b = (0, a)\\).\n\n(b) \\(a = (0, b)\\).\n\n(c) \\(b = (0, a, 0)\\).\n\n(d) \\(a = 0 = b\\).\n\n**1.4 Periodic energy usage.** The 168-vector \\(w\\) gives the hourly electricity consumption of a manufacturing plant, starting on Sunday midnight to 1AM, over one week, in MWh (megawatt-hours). The consumption pattern is the same each day, i.e., it is 24-periodic, which means that \\(w_{t+24} = w_t\\) for \\(t = 1, \\ldots, 144\\). Let \\(d\\) be the 24-vector that gives the energy consumption over one day, starting at midnight.\n\n(a) Use vector notation to express \\(w\\) in terms of \\(d\\).\n\n(b) Use vector notation to express \\(d\\) in terms of \\(w\\).\n\n**1.5 Interpreting sparsity.** Suppose the \\(n\\)-vector \\(x\\) is sparse, i.e., has only a few nonzero entries. Give a short sentence or two explaining what this means in each of the following contexts.\n\n(a) \\(x\\) represents the daily cash flow of some business over \\(n\\) days.\n\n(b) \\(x\\) represents the annual dollar value purchases by a customer of \\(n\\) products or services.\n\n(c) \\(x\\) represents a portfolio, say, the dollar value holdings of \\(n\\) stocks.\n\n(d) \\(x\\) represents a bill of materials for a project, i.e., the amounts of \\(n\\) materials needed.\n\n(e) \\(x\\) represents a monochrome image, i.e., the brightness values of \\(n\\) pixels.\n\n(f) \\(x\\) is the daily rainfall in a location over one year.\n\n**1.6 Vector of differences.** Suppose \\(x\\) is an \\(n\\)-vector. The associated vector of differences is the \\((n-1)\\)-vector \\(d\\) given by \\(d = (x_2 - x_1, x_3 - x_2, \\ldots, x_n - x_{n-1})\\). Express \\(d\\) in terms of \\(x\\) using vector operations (e.g., slicing notation, sum, difference, linear combinations, inner product). The difference vector has a simple interpretation when \\(x\\) represents a time series. For example, if \\(x\\) gives the daily value of some quantity, \\(d\\) gives the day-to-day changes in the quantity.\n\n**1.7 Transforming between two encodings for Boolean vectors.** A Boolean \\(n\\)-vector is one for which all entries are either 0 or 1. Such vectors are used to encode whether each of \\(n\\) conditions holds, with \\(a_i = 1\\) meaning that condition \\(i\\) holds. Another common encoding of the same information uses the two values \\(-1\\) and \\(+1\\) for the entries. For example, the Boolean vector \\((0, 1, 1, 0)\\) would be written using this alternative encoding as \\((-1, +1, +1, -1)\\). Suppose that \\(x\\) is a Boolean vector with entries that are 0 or 1, and \\(y\\) is a vector encoding the same information using the values \\(-1\\) and \\(+1\\). Express \\(y\\) in terms of \\(x\\) using vector notation. Also, express \\(x\\) in terms of \\(y\\) using vector notation.\n\n**1.8 Profit and sales vectors.** A company sells \\(n\\) different products or items. The \\(n\\)-vector \\(p\\) gives the profit, in dollars per unit, for each of the \\(n\\) items. (The entries of \\(p\\) are typically positive, but a few items might have negative entries. These items are called loss leaders, and are used to increase customer engagement in the hope that the customer will make other, profitable purchases.) The \\(n\\)-vector \\(s\\) gives the total sales of each of the items, over some period (such as a month), i.e., \\(s_i\\) is the total number of units of item \\(i\\) sold. (These are also typically nonnegative, but negative entries can be used to reflect items that were purchased in a previous time period and returned in this one.) Express the total profit in terms of \\(p\\) and \\(s\\) using vector notation.\n\n**1.9 Symptoms vector.** A 20-vector \\(s\\) records whether each of 20 different symptoms is present in a medical patient, with \\(s_i = 1\\) meaning the patient has the symptom and \\(s_i = 0\\) meaning she does not. Express the following using vector notation.\n\n(a) The total number of symptoms the patient has.\n\n(b) The patient exhibits five out of the first ten symptoms.\n\n**1.10 Total score from course record.** The record for each student in a class is given as a 10-vector \\(r\\), where \\(r_1, \\ldots, r_8\\) are the grades for the 8 homework assignments, each on a 0\u201310 scale, \\(r_9\\) is the midterm exam grade on a 0\u2013120 scale, and \\(r_{10}\\) is final",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Chapter 3: Norm and distance",
                    "content": null,
                    "children": [
                        {
                            "name": "3.1 Norm",
                            "content": "The Euclidean norm of a vector, often simply called the norm, is a measure of the vector's magnitude. For a vector \\( x \\) with elements \\( x_1, x_2, \\ldots, x_n \\), the Euclidean norm is defined as:\n\n\\[ \n\\|x\\| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} \n\\]\n\nThis can also be expressed using the inner product as:\n\n\\[ \n\\|x\\| = \\sqrt{x^T x} \n\\]\n\nwhere \\( x^T \\) denotes the transpose of the vector \\( x \\). The Euclidean norm is sometimes denoted as \\( \\|x\\|_2 \\).\n\nProperties of the Euclidean norm include:\n\n1. **Nonnegative Homogeneity**: For any scalar \\( \\alpha \\), \\( \\|\\alpha x\\| = |\\alpha| \\|x\\| \\).\n2. **Triangle Inequality**: For any vectors \\( x \\) and \\( y \\), \\( \\|x + y\\| \\leq \\|x\\| + \\|y\\| \\).\n3. **Nonnegativity**: \\( \\|x\\| \\geq 0 \\).\n4. **Definiteness**: \\( \\|x\\| = 0 \\) if and only if \\( x = 0 \\).\n\nThe norm provides a way to define the distance between two vectors \\( a \\) and \\( b \\) as the norm of their difference:\n\n\\[ \n\\text{dist}(a, b) = \\|a - b\\| \n\\]\n\nThis distance is the Euclidean distance, which is a measure of how far apart the vectors are in the space they inhabit.\n\nThe norm also relates to the concept of the root-mean-square (RMS) value of a vector. The RMS value of a vector \\( x \\) is defined as:\n\n\\[ \n\\text{rms}(x) = \\frac{\\|x\\|}{\\sqrt{n}} \n\\]\n\nwhere \\( n \\) is the number of elements in the vector. The RMS value provides a measure of the typical size of the elements of the vector.\n\nThe Chebyshev inequality is a useful result that states if \\( k \\) of the entries of a vector \\( x \\) satisfy \\( |x_i| \\geq a \\), then:\n\n\\[ \nk \\leq \\frac{\\|x\\|^2}{a^2} \n\\]\n\nThis inequality provides a bound on the number of entries of a vector that can be large relative to the norm of the vector.",
                            "children": []
                        },
                        {
                            "name": "3.2 Distance",
                            "content": "Certainly! Here's the text for section 3.2 Distance:\n\nDistance\n\nEuclidean distance. We can use the norm to define the Euclidean distance between two vectors \\( \\mathbf{a} \\) and \\( \\mathbf{b} \\) as the norm of their difference:\n\n\\[ \\text{dist}(\\mathbf{a}, \\mathbf{b}) = \\|\\mathbf{a} - \\mathbf{b}\\|. \\]\n\nFor one, two, and three dimensions, this distance is exactly the usual distance between points with coordinates \\( \\mathbf{a} \\) and \\( \\mathbf{b} \\), as illustrated in figure 3.1. But the Euclidean distance is defined for vectors of any dimension; we can refer to the distance between two vectors of dimension 100. Since we only use the Euclidean norm in this book, we will refer to the Euclidean distance between vectors as, simply, the distance between the vectors. If \\( \\mathbf{a} \\) and \\( \\mathbf{b} \\) are \\( n \\)-vectors, we refer to the RMS value of the difference, \\( \\|\\mathbf{a} - \\mathbf{b}\\|/\\sqrt{n} \\), as the RMS deviation between the two vectors.\n\nWhen the distance between two \\( n \\)-vectors \\( \\mathbf{x} \\) and \\( \\mathbf{y} \\) is small, we say they are 'close' or 'nearby', and when the distance \\( \\|\\mathbf{x} - \\mathbf{y}\\| \\) is large, we say they are 'far'. The particular numerical values of \\( \\|\\mathbf{x} - \\mathbf{y}\\| \\) that correspond to 'close' or 'far' depend on the particular application.\n\nAs an example, consider the 4-vectors\n\n\\[ \\mathbf{u} = \\begin{bmatrix} 1.8 \\\\ 2.0 \\\\ -3.7 \\\\ 4.7 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} 0.6 \\\\ 2.1 \\\\ 1.9 \\\\ -1.4 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 2.0 \\\\ 1.9 \\\\ -4.0 \\\\ 4.6 \\end{bmatrix}. \\]\n\nThe distances between pairs of them are\n\n\\[ \\|\\mathbf{u} - \\mathbf{v}\\| = 8.368, \\quad \\|\\mathbf{u} - \\mathbf{w}\\| = 0.387, \\quad \\|\\mathbf{v} - \\mathbf{w}\\| = 8.533, \\]\n\nso we can say that \\( \\mathbf{u} \\) is much nearer (or closer) to \\( \\mathbf{w} \\) than it is to \\( \\mathbf{v} \\). We can also say that \\( \\mathbf{w} \\) is much nearer to \\( \\mathbf{u} \\) than it is to \\( \\mathbf{v} \\).\n\nTriangle inequality. We can now explain where the triangle inequality gets its name. Consider a triangle in two or three dimensions, whose vertices have coordinates \\( \\mathbf{a} \\), \\( \\mathbf{b} \\), and \\( \\mathbf{c} \\). The lengths of the sides are the distances between the vertices,\n\n\\[ \\text{dist}(\\mathbf{a}, \\mathbf{b}) = \\|\\mathbf{a} - \\mathbf{b}\\|, \\quad \\text{dist}(\\mathbf{b}, \\mathbf{c}) = \\|\\mathbf{b} - \\mathbf{c}\\|, \\quad \\text{dist}(\\mathbf{a}, \\mathbf{c}) = \\|\\mathbf{a} - \\mathbf{c}\\|. \\]\n\nGeometric intuition tells us that the length of any side of a triangle cannot exceed the sum of the lengths of the other two sides. For example, we have\n\n\\[ \\|\\mathbf{a} - \\mathbf{c}\\| \\leq \\|\\mathbf{a} - \\mathbf{b}\\| + \\|\\mathbf{b} - \\mathbf{c}\\|. \\]\n\nThis follows from the triangle inequality, since\n\n\\[ \\|\\mathbf{a} - \\mathbf{c}\\| = \\|(\\mathbf{a} - \\mathbf{b}) + (\\mathbf{b} - \\mathbf{c})\\| \\leq \\|\\mathbf{a} - \\mathbf{b}\\| + \\|\\mathbf{b} - \\mathbf{c}\\|. \\]\n\nThis is illustrated in figure 3.2.",
                            "children": []
                        },
                        {
                            "name": "3.3 Standard deviation",
                            "content": "Certainly! Here is the text for section 3.3, Standard deviation:\n\n---\n\n3.3 Standard deviation\n\nFor any vector \\( x \\), the vector \\(\\tilde{x} = x - \\text{avg}(x)1\\) is called the associated de-meaned vector, obtained by subtracting from each entry of \\( x \\) the mean value of the entries. (This is not standard notation; i.e., \\(\\tilde{x}\\) is not generally used to denote the de-meaned vector.) The mean value of the entries of \\(\\tilde{x}\\) is zero, i.e., \\(\\text{avg}(\\tilde{x}) = 0\\). This explains why \\(\\tilde{x}\\) is called the de-meaned version of \\( x \\); it is \\( x \\) with its mean removed. The de-meaned vector is useful for understanding how the entries of a vector deviate from their mean value. It is zero if all the entries in the original vector \\( x \\) are the same.\n\nThe standard deviation of an \\( n \\)-vector \\( x \\) is defined as the RMS value of the de-meaned vector \\( x - \\text{avg}(x)1 \\), i.e.,\n\n\\[\n\\text{std}(x) = \\sqrt{\\frac{(x_1 - \\text{avg}(x))^2 + \\cdots + (x_n - \\text{avg}(x))^2}{n}}\n\\]\n\nThis is the same as the RMS deviation between a vector \\( x \\) and the vector all of whose entries are \\(\\text{avg}(x)\\). It can be written using the inner product and norm as\n\n\\[\n\\text{std}(x) = \\frac{\\|x - (\\frac{1^T x}{n})1\\|}{\\sqrt{n}}\n\\]\n\nThe standard deviation of a vector \\( x \\) tells us the typical amount by which its entries deviate from their average value. The standard deviation of a vector is zero only when all its entries are equal. The standard deviation of a vector is small when the entries of the vector are nearly the same.\n\nAs a simple example consider the vector \\( x = (1, -2, 3, 2) \\). Its mean or average value is \\(\\text{avg}(x) = 1\\), so the de-meaned vector is \\(\\tilde{x} = (0, -3, 2, 1)\\). Its standard deviation is \\(\\text{std}(x) = 1.872\\). We interpret this number as a 'typical' value by which the entries differ from the mean of the entries. These numbers are 0, 3, 2, and 1, so 1.872 is reasonable.\n\nWe should warn the reader that another slightly different definition of the standard deviation of a vector is widely used, in which the denominator \\(\\sqrt{n}\\) in (3.4) is replaced with \\(\\sqrt{n-1}\\) (for \\(n \\geq 2\\)). In this book we will only use the definition (3.4).\n\nIn some applications the Greek letter \\(\\sigma\\) (sigma) is traditionally used to denote standard deviation, while the mean is denoted \\(\\mu\\) (mu). In this notation we have, for an \\( n \\)-vector \\( x \\),\n\n\\[\n\\mu = \\frac{1^T x}{n}; \\quad \\sigma = \\frac{\\|x - \\mu 1\\|}{\\sqrt{n}}\n\\]\n\nWe will use the symbols \\(\\text{avg}(x)\\) and \\(\\text{std}(x)\\), switching to \\(\\mu\\) and \\(\\sigma\\) only with explanation, when describing an application that traditionally uses these symbols.\n\nAverage, RMS value, and standard deviation. The average, RMS value, and standard deviation of a vector are related by the formula\n\n\\[\n\\text{rms}(x)^2 = \\text{avg}(x)^2 + \\text{std}(x)^2\n\\]\n\nThis formula makes sense: \\(\\text{rms}(x)^2\\) is the mean square value of the entries of \\( x \\), which can be expressed as the square of the mean value, plus the mean square fluctuation of the entries of \\( x \\) around their mean value. We can derive this formula from our vector notation formula for \\(\\text{std}(x)\\) given above. We have\n\n\\[\n\\text{std}(x)^2 = \\frac{1}{n}\\|x - (\\frac{1^T x}{n})1\\|^2 = \\frac{1}{n}(x^T x - 2x^T (\\frac{1^T x}{n})1 + ((\\frac{1^T x}{n})1)^T ((\\frac{1^T x}{n})1))\n\\]\n\n\\[\n= \\frac{1}{n}(x^T x - \\frac{2}{n}(1^T x)^2 + n(\\frac{1^T x}{n})^2)\n\\]\n\n\\[\n= \\frac{1}{n}x^T x - (\\frac{1^T x}{n})^2\n\\]\n\n\\[\n= \\text{rms}(x)^2 - \\text{avg}(x)^2\n\\]\n\nwhich can be re-arranged to obtain the identity (3.5) above. This derivation uses many of the properties for norms and inner products, and should be read carefully to understand every step. In the second line, we expand the norm-square of the sum of two vectors. In the third line, we use the commutative property of scalar-vector multiplication, moving scalars such as \\((\\frac{1^T x}{n})\\) to the front of each term, and also the fact that \\(1^T 1 = n\\).\n\nExamples.\n\n- Mean return and risk. Suppose that an \\( n \\)-vector represents a time series of return on an investment, expressed as a percentage, in \\( n \\) time periods over some interval of time. Its average gives the mean return over the whole interval, often shortened to its return. Its standard deviation is a measure of how variable the return is, from period to period, over the time interval, i.e., how much it typically varies from its mean, and is often called the (per period) risk of the investment. Multiple investments can be compared by plotting them on a risk-return plot, which gives the mean and standard deviation of the returns of each of the investments over some interval. A desirable return history vector has high mean return and low risk; this means that the returns in the different periods are consistently high. Figure 3.4 shows an example.\n\n- Temperature or rainfall. Suppose that an \\( n \\)-vector is a time series of the daily average temperature at a particular location, over a one year period. Its average gives the average temperature at that location (over the year) and its standard",
                            "children": []
                        },
                        {
                            "name": "3.4 Angle",
                            "content": "Here is the text for section 3.4 Angle:\n\n---\n\n**3.4 Angle**\n\n**Cauchy\u2013Schwarz inequality.** An important inequality that relates norms and inner products is the Cauchy\u2013Schwarz inequality:\n\n\\[\n|a^T b| \\leq \\|a\\| \\|b\\|\n\\]\n\nfor any \\(n\\)-vectors \\(a\\) and \\(b\\). Written out in terms of the entries, this is\n\n\\[\n|a_1b_1 + \\cdots + a_n b_n| \\leq \\sqrt{a_1^2 + \\cdots + a_n^2} \\sqrt{b_1^2 + \\cdots + b_n^2},\n\\]\n\nwhich looks more intimidating. This inequality is attributed to the mathematician Augustin-Louis Cauchy; Hermann Schwarz gave the derivation given below.\n\nThe Cauchy\u2013Schwarz inequality can be shown as follows. The inequality clearly holds if \\(a = 0\\) or \\(b = 0\\) (in this case, both sides of the inequality are zero). So we suppose now that \\(a \\neq 0\\), \\(b \\neq 0\\), and define \\(\\beta = \\|a\\|\\), \\(\\gamma = \\|b\\|\\). We observe that\n\n\\[\n0 \\leq \\| \\gamma a - \\beta b \\|^2\n= \\| \\gamma a \\|^2 - 2(\\gamma a)^T (\\beta b) + \\| \\beta b \\|^2\n= \\gamma^2 \\| a \\|^2 - 2 \\gamma \\beta (a^T b) + \\beta^2 \\| b \\|^2\n= \\| b \\|^2 \\| a \\|^2 - 2 \\| b \\| \\| a \\| (a^T b) + \\| a \\|^2 \\| b \\|^2\n= 2 \\| a \\|^2 \\| b \\|^2 - 2 \\| a \\| \\| b \\| (a^T b).\n\\]\n\nDividing by \\(2 \\| a \\| \\| b \\|\\) yields \\(a^T b \\leq \\| a \\| \\| b \\|\\). Applying this inequality to \\(-a\\) and \\(b\\) we obtain \\(-a^T b \\leq \\| a \\| \\| b \\|\\). Putting these two inequalities together we get the Cauchy\u2013Schwarz inequality, \\(|a^T b| \\leq \\| a \\| \\| b \\|\\).\n\nThis argument also reveals the conditions on \\(a\\) and \\(b\\) under which they satisfy the Cauchy\u2013Schwarz inequality with equality. This occurs only if \\(\\| \\gamma a - \\beta b \\| = 0\\), i.e., \\(\\gamma a = \\beta b\\). This means that each vector is a scalar multiple of the other (in the case when they are nonzero). This statement remains true when either \\(a\\) or \\(b\\) is zero. So the Cauchy\u2013Schwarz inequality holds with equality when one of the vectors is a multiple of the other; in all other cases, it holds with strict inequality.\n\n**Verification of triangle inequality.** We can use the Cauchy\u2013Schwarz inequality to verify the triangle inequality. Let \\(a\\) and \\(b\\) be any vectors. Then\n\n\\[\n\\| a + b \\|^2 = \\| a \\|^2 + 2 a^T b + \\| b \\|^2\n\\leq \\| a \\|^2 + 2 \\| a \\| \\| b \\| + \\| b \\|^2\n= (\\| a \\| + \\| b \\|)^2,\n\\]\n\nwhere we used the Cauchy\u2013Schwarz inequality in the second line. Taking the square root we get the triangle inequality, \\(\\| a + b \\| \\leq \\| a \\| + \\| b \\|\\).\n\n**Angle between vectors.** The angle between two nonzero vectors \\(a, b\\) is defined as\n\n\\[\n\\theta = \\arccos \\left( \\frac{a^T b}{\\| a \\| \\| b \\|} \\right)\n\\]\n\nwhere \\(\\arccos\\) denotes the inverse cosine, normalized to lie in the interval \\([0, \\pi]\\). In other words, we define \\(\\theta\\) as the unique number between \\(0\\) and \\(\\pi\\) that satisfies\n\n\\[\na^T b = \\| a \\| \\| b \\| \\cos \\theta.\n\\]\n\nThe angle between \\(a\\) and \\(b\\) is written as \\(\\angle (a, b)\\), and is sometimes expressed in degrees. (The default angle unit is radians; \\(360^\\circ\\) is \\(2\\pi\\) radians.) For example, \\(\\angle (a, b) = 60^\\circ\\) means \\(\\angle (a, b) = \\pi/3\\), i.e., \\(a^T b = (1/2) \\| a \\| \\| b \\|\\).\n\nThe angle coincides with the usual notion of angle between vectors, when they have dimension two or three, and they are thought of as displacements from a common point. For example, the angle between the vectors \\(a = (1, 2, -1)\\) and \\(b = (2, 0, -3)\\) is\n\n\\[\n\\arccos \\left( \\frac{5}{\\sqrt{6} \\sqrt{13}} \\right) = \\arccos(0.5661) = 0.9690 = 55.52^\\circ\n\\]\n\n(to 4 digits). But the definition of angle is more general; we can refer to the angle between two vectors with dimension 100.\n\nThe angle is a symmetric function of \\(a\\) and \\(b\\): We have \\(\\angle (a, b) = \\angle (b, a)\\). The angle is not affected by scaling each of the vectors by a positive scalar: We have, for any vectors \\(a\\) and \\(b\\), and any positive numbers \\(\\beta\\) and \\(\\gamma\\),\n\n\\[\n\\angle (\\beta a, \\gamma b) = \\angle (a, b).\n\\]\n\n**Acute and obtuse angles.** Angles are classified according to the sign of \\(a^T b\\). Suppose \\(a\\) and \\(b\\) are nonzero vectors of the same size.\n\n- If the angle is \\(\\pi/2 = 90^\\circ\\), i.e., \\(a^T b = 0\\), the vectors are said to be orthogonal. We write \\(a \\perp b\\) if \\(a\\) and \\(b\\) are orthogonal. (By convention",
                            "children": []
                        },
                        {
                            "name": "3.5 Complexity",
                            "content": "Certainly! Here's the text for section 3.5 Complexity:\n\n---\n\n3.5 Complexity\n\nThe complexity of computing the norm of an n-vector x is n flops, since it involves n multiplications and n-1 additions (the additions required to compute the sum of squares). The complexity of computing the distance between two n-vectors x and y is 2n flops, since it involves n subtractions, n multiplications, and n-1 additions.\n\nThe complexity of computing the standard deviation of an n-vector is 3n flops: n-1 flops to compute the mean, n flops to compute the de-meaned vector, n flops to compute the sum of squares of the de-meaned vector, and one division and one square root. The complexity of computing the angle between two n-vectors is 4n flops: 2n flops to compute the inner product of the vectors, and 2n flops to compute the norms of the vectors.\n\nIn all these cases, the order of the computation is n, which means that the time required scales linearly with the dimension n of the vectors involved. This is typical for vector operations.",
                            "children": []
                        },
                        {
                            "name": "Exercises",
                            "content": "Exercises\n\n1.1 Vector equations. Determine whether each of the equations below is true, false, or contains bad notation (and therefore does not make sense).  \n(a) \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} = (1, 2, 1)\\).  \n(b) \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}\\).  \n(c) (1, (2, 1)) = ((1, 2), 1).\n\n1.2 Vector notation. Which of the following expressions uses correct notation? When the expression does make sense, give its length. In the following, \\(a\\) and \\(b\\) are 10-vectors, and \\(c\\) is a 20-vector.  \n(a) \\(a + b - c\\).  \n(b) \\((a, b, c)\\).  \n(c) \\(2a + c\\).  \n(d) \\((a, 1) + (c_1, b)\\).  \n(e) \\(((a, b), a)\\).  \n(f) \\([a \\, b] + 4c\\).  \n(g) \\(\\begin{pmatrix} a \\\\ b \\end{pmatrix} + 4c\\).\n\n1.3 Overloading. Which of the following expressions uses correct notation? If the notation is correct, is it also unambiguous? Assume that \\(a\\) is a 10-vector and \\(b\\) is a 20-vector.  \n(a) \\(b = (0, a)\\).  \n(b) \\(a = (0, b)\\).  \n(c) \\(b = (0, a, 0)\\).  \n(d) \\(a = 0 = b\\).\n\n1.4 Periodic energy usage. The 168-vector \\(w\\) gives the hourly electricity consumption of a manufacturing plant, starting on Sunday midnight to 1AM, over one week, in MWh (megawatt-hours). The consumption pattern is the same each day, i.e., it is 24-periodic, which means that \\(w_{t+24} = w_t\\) for \\(t = 1, \\ldots, 144\\). Let \\(d\\) be the 24-vector that gives the energy consumption over one day, starting at midnight.  \n(a) Use vector notation to express \\(w\\) in terms of \\(d\\).  \n(b) Use vector notation to express \\(d\\) in terms of \\(w\\).\n\n1.5 Interpreting sparsity. Suppose the \\(n\\)-vector \\(x\\) is sparse, i.e., has only a few nonzero entries. Give a short sentence or two explaining what this means in each of the following contexts.  \n(a) \\(x\\) represents the daily cash flow of some business over \\(n\\) days.  \n(b) \\(x\\) represents the annual dollar value purchases by a customer of \\(n\\) products or services.  \n(c) \\(x\\) represents a portfolio, say, the dollar value holdings of \\(n\\) stocks.  \n(d) \\(x\\) represents a bill of materials for a project, i.e., the amounts of \\(n\\) materials needed.  \n(e) \\(x\\) represents a monochrome image, i.e., the brightness values of \\(n\\) pixels.  \n(f) \\(x\\) is the daily rainfall in a location over one year.\n\n1.6 Vector of differences. Suppose \\(x\\) is an \\(n\\)-vector. The associated vector of differences is the \\((n-1)\\)-vector \\(d\\) given by \\(d = (x_2 - x_1, x_3 - x_2, \\ldots, x_n - x_{n-1})\\). Express \\(d\\) in terms of \\(x\\) using vector operations (e.g., slicing notation, sum, difference, linear combinations, inner product). The difference vector has a simple interpretation when \\(x\\) represents a time series. For example, if \\(x\\) gives the daily value of some quantity, \\(d\\) gives the day-to-day changes in the quantity.\n\n1.7 Transforming between two encodings for Boolean vectors. A Boolean \\(n\\)-vector is one for which all entries are either 0 or 1. Such vectors are used to encode whether each of \\(n\\) conditions holds, with \\(a_i = 1\\) meaning that condition \\(i\\) holds. Another common encoding of the same information uses the two values \\(-1\\) and \\(+1\\) for the entries. For example the Boolean vector \\((0, 1, 1, 0)\\) would be written using this alternative encoding as \\((-1, +1, +1, -1)\\). Suppose that \\(x\\) is a Boolean vector with entries that are 0 or 1, and \\(y\\) is a vector encoding the same information using the values \\(-1\\) and \\(+1\\). Express \\(y\\) in terms of \\(x\\) using vector notation. Also, express \\(x\\) in terms of \\(y\\) using vector notation.\n\n1.8 Profit and sales vectors. A company sells \\(n\\) different products or items. The \\(n\\)-vector \\(p\\) gives the profit, in dollars per unit, for each of the \\(n\\) items. (The entries of \\(p\\) are typically positive, but a few items might have negative entries. These items are called loss leaders, and are used to increase customer engagement in the hope that the customer will make other, profitable purchases.) The \\(n\\)-vector \\(s\\) gives the total sales of each of the items, over some period (such as a month), i.e., \\(s_i\\) is the total number of units of item \\(i\\) sold. (These are also typically nonnegative, but negative entries can be used to reflect items that were purchased in a previous time period and returned in this one.) Express the total profit in terms of \\(p\\) and \\(s\\) using vector notation.\n\n1.9 Symptoms vector. A 20-vector \\(s\\) records whether each of 20 different symptoms is present in a medical patient, with \\(s_i = 1\\) meaning the patient has the symptom and \\(s_i = 0\\) meaning she does not. Express the following using vector notation.  \n(a) The total number of symptoms the patient has.  \n(b) The patient exhibits five out of the first ten symptoms.\n\n1.10 Total score from course record. The record for each student in a class is given as a 10-vector \\(r\\), where \\(r_1, \\ldots, r_8\\) are the grades for the 8 homework assignments, each on a 0\u201310 scale, \\(r_9\\) is the midterm exam grade on a 0\u2013120 scale, and \\(r_{10}\\) is",
                            "children": []
                        }
                    ]
                }
            ]
        }
    ]
}